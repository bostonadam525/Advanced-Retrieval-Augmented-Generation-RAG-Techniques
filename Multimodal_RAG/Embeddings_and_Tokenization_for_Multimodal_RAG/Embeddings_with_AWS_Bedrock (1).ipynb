{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XD8MB7mKNEd"
   },
   "source": [
    "# Embeddings with AWS Bedrock\n",
    "* Notebook by Adam Lang\n",
    "* Date: 2/21/2025\n",
    "\n",
    "# Overview\n",
    "* We will use a pre-trained embedding model from AWS Bedrock to create embeddings.\n",
    "\n",
    "# AWS Titan Text Embeddings\n",
    "* Titan Text Embeddings docs: https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html\n",
    "* Configuration:\n",
    "  * can intake up to 8,192 tokens\n",
    "  * outputs a vector of 1,024 dimensions\n",
    "  * The model is optimized for text retrieval tasks, but can also be optimized for additional tasks, such as semantic similarity and clustering.\n",
    "```\n",
    "Amazon Titan Text Embeddings V2 model\n",
    "\n",
    "Model ID - amazon.titan-embed-text-v2:0\n",
    "\n",
    "Max input text tokens  8,192\n",
    "\n",
    "Languages - English (100+ languages in preview)\n",
    "\n",
    "Output vector size - 1,024 (default), 512, 256\n",
    "\n",
    "Inference types – On-Demand, Provisioned Throughput\n",
    "\n",
    "Supported use cases – RAG, document search, reranking, classification, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl2MYullLXQa"
   },
   "source": [
    "# Create Bedrock Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZoM0dt9MKJoI"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cIKDNBYlLZ_0"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "## create bedrock client\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\")\n",
    "\n",
    "## create bedrock runtime\n",
    "bedrock_runtime_client = boto3.client(service_name=\"bedrock-runtime\",\n",
    "                                      region_name=\"eu-central-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWWP0B8hMZaB"
   },
   "source": [
    "# List Foundation Models\n",
    "* We can easily list the foundation models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "33QZ-tOeLr9o",
    "outputId": "de2f1dfd-9977-46f7-ad1f-4f6cb800b8fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '4d90aa03-e81c-49ae-8c6d-50ba269a4c17',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Fri, 21 Feb 2025 20:45:39 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '11593',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '4d90aa03-e81c-49ae-8c6d-50ba269a4c17'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.titan-text-express-v1:0:8k',\n",
       "   'modelId': 'amazon.titan-text-express-v1:0:8k',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.titan-text-express-v1',\n",
       "   'modelId': 'amazon.titan-text-express-v1',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelId': 'amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.titan-text-lite-v1',\n",
       "   'modelId': 'amazon.titan-text-lite-v1',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.titan-embed-text-v1',\n",
       "   'modelId': 'amazon.titan-embed-text-v1',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.titan-embed-image-v1:0',\n",
       "   'modelId': 'amazon.titan-embed-image-v1:0',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.titan-embed-image-v1',\n",
       "   'modelId': 'amazon.titan-embed-image-v1',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.titan-embed-text-v2:0',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0',\n",
       "   'modelName': 'Titan Embeddings G2 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/amazon.rerank-v1:0',\n",
       "   'modelId': 'amazon.rerank-v1:0',\n",
       "   'modelName': 'Rerank 1.0',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/anthropic.claude-instant-v1',\n",
       "   'modelId': 'anthropic.claude-instant-v1',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/anthropic.claude-v2:1:18k',\n",
       "   'modelId': 'anthropic.claude-v2:1:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/anthropic.claude-v2:1:200k',\n",
       "   'modelId': 'anthropic.claude-v2:1:200k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/anthropic.claude-v2:1',\n",
       "   'modelId': 'anthropic.claude-v2:1',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/anthropic.claude-v2',\n",
       "   'modelId': 'anthropic.claude-v2',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/cohere.embed-english-v3',\n",
       "   'modelId': 'cohere.embed-english-v3',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/cohere.embed-multilingual-v3',\n",
       "   'modelId': 'cohere.embed-multilingual-v3',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/cohere.rerank-v3-5:0',\n",
       "   'modelId': 'cohere.rerank-v3-5:0',\n",
       "   'modelName': 'Rerank 3.5',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/meta.llama3-2-1b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-2-1b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3.2 1B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:eu-central-1::foundation-model/meta.llama3-2-3b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-2-3b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3.2 3B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_client.list_foundation_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bZHdQgoMeO9"
   },
   "source": [
    "Summary\n",
    "* We can see above that the metadata tells us things such as the input data type, if it can be fine-tuned, and the model lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon.titan-text-express-v1:0:8k',\n",
       " 'amazon.titan-text-express-v1',\n",
       " 'amazon.titan-text-lite-v1:0:4k',\n",
       " 'amazon.titan-text-lite-v1',\n",
       " 'amazon.titan-embed-text-v1:2:8k',\n",
       " 'amazon.titan-embed-text-v1',\n",
       " 'amazon.titan-embed-image-v1:0',\n",
       " 'amazon.titan-embed-image-v1',\n",
       " 'amazon.titan-embed-text-v2:0',\n",
       " 'amazon.rerank-v1:0',\n",
       " 'anthropic.claude-instant-v1',\n",
       " 'anthropic.claude-v2:1:18k',\n",
       " 'anthropic.claude-v2:1:200k',\n",
       " 'anthropic.claude-v2:1',\n",
       " 'anthropic.claude-v2',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'cohere.embed-english-v3',\n",
       " 'cohere.embed-multilingual-v3',\n",
       " 'cohere.rerank-v3-5:0',\n",
       " 'meta.llama3-2-1b-instruct-v1:0',\n",
       " 'meta.llama3-2-3b-instruct-v1:0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## list specific LLM models\n",
    "all_models = [model['modelId'] for model in bedrock_client.list_foundation_models()['modelSummaries']]\n",
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon.titan-embed-text-v1:2:8k',\n",
       " 'amazon.titan-embed-text-v1',\n",
       " 'amazon.titan-embed-image-v1:0',\n",
       " 'amazon.titan-embed-image-v1',\n",
       " 'amazon.titan-embed-text-v2:0',\n",
       " 'cohere.embed-english-v3',\n",
       " 'cohere.embed-multilingual-v3']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## list embedding models\n",
    "embedding_models = [model for model in all_models if 'embed' in model.lower()]\n",
    "embedding_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings with AWS Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding vector has 1024 values\n",
      "[-5.40625, 1.28125, 3.03125, '...', -0.71875, -2.953125, -0.51953125]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "## prompt\n",
    "prompt = \"Hello, welcome to AWS Bedrock, I hope you find what you are looking for!\"\n",
    "\n",
    "# ## 2nd prompt \n",
    "# prompt2 = \"\"\"AWS Bedrock supports foundation models from industry-leadin providers such as\n",
    "# A21 labs, Anthropic, Stability AI, and Amazon. Choose the model that is best suited for your use case and goals!\n",
    "# \"\"\"\n",
    "\n",
    "## init model_id from AWS bedrock --- can use different model_id's \n",
    "model_id = 'amazon.titan-embed-text-v2:0' \n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt, \n",
    "    \"dimensions\": 1024, ## other sizes --> 384, 256\n",
    "    \"normalize\": False\n",
    "    \n",
    "})\n",
    "# Invoke the model\n",
    "response = bedrock_runtime_client.invoke_model(\n",
    "    body=body,\n",
    "    modelId=model_id,\n",
    "    accept='application/json',\n",
    "    contentType='application/json'\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "response_body = json.loads(response['body'].read())\n",
    "\n",
    "# Get embeddings\n",
    "embedding = response_body['embedding']\n",
    "print(f\"The embedding vector has {len(embedding)} values\\n{embedding[:3]+['...']+embedding[-3:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.168456186155574"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check vector magnitude to see if normalized\n",
    "import numpy as np \n",
    "\n",
    "## calculate magnitude\n",
    "np.linalg.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "* We can see the length of the vector embeddings above is NOT equal to 1 it is quite high thus we have validated the fact that the embeddings were not normalized since we set the normalize function to FALSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Parameter in Embeddings\n",
    "* Normalizing a vector is the process of scaling it to have a unit length of magnitude of 1.\n",
    "* This is pretty useful to ensure that all vectors have the `same scale` and contribute equally during vector operations preventing some vectors from dominating others due to their larger magnitudes.\n",
    "* This is usually done in data pre-processing.\n",
    "\n",
    "## When to normalize?\n",
    "* Use as default for MOST use cases such as: Retrieval, RAG, related use cases.\n",
    "\n",
    "## When NOT to normalize?\n",
    "* Normalization will work for most use cases but there may be some cases such as `Classification` or `Entity extraction` where it may not work well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Normalizing Embeddings with Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding vector has 1024 values\n",
      "[-0.08297207951545715, 0.04197411239147186, -0.04392639547586441, '...', -0.0021149746607989073, -0.04229949042201042, 0.005490799434483051]\n"
     ]
    }
   ],
   "source": [
    "## example of normalization in embedding generation\n",
    "prompt = \"\"\"AWS Bedrock supports foundation models from industry-leadin providers such as\n",
    "A21 labs, Anthropic, Stability AI, and Amazon. Choose the model that is best suited for your use case and goals!\n",
    "\"\"\"\n",
    "\n",
    "## init model_id from AWS bedrock --- can use different model_id's \n",
    "model_id = 'amazon.titan-embed-text-v2:0' \n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt, \n",
    "    \"dimensions\": 1024, ## other sizes --> 384, 256\n",
    "    \"normalize\": True, ## set this to True to NORMALIZE vectors\n",
    "    \n",
    "})\n",
    "# Invoke the model\n",
    "response = bedrock_runtime_client.invoke_model(\n",
    "    body=body,\n",
    "    modelId=model_id,\n",
    "    accept='application/json',\n",
    "    contentType='application/json'\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "response_body = json.loads(response['body'].read())\n",
    "\n",
    "# Get embeddings\n",
    "embedding = response_body['embedding']\n",
    "print(f\"The embedding vector has {len(embedding)} values\\n{embedding[:3]+['...']+embedding[-3:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999543085568"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check vector magnitude to see if normalized\n",
    "import numpy as np \n",
    "\n",
    "## calculate magnitude\n",
    "np.linalg.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "* We can see the value rounds up to 1.0 which means that we have normalized the embeddings.\n",
    "* The main point is that not all embeddings are created equal in a vector space and this depends on the language of origin, special characters, length of the sentences, number of tokens and other variables. So normalization is a scaling step in feature engineering, however it can skew the results so be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Cosine Similarity\n",
    "* We can now test the similarity of 2 texts/prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(prompt):\n",
    "    body = json.dumps({\n",
    "        \"inputText\": prompt,\n",
    "        \"dimensions\": 1024,\n",
    "        \"normalize\": False,\n",
    "    })\n",
    "    model = bedrock_runtime_client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=body, \n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(model.get('body').read())\n",
    "\n",
    "    return response_body.get(\"embedding\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'Python is a programming language.' and 'I am going to the moon.': -0.012073559764820775\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text1 = \"Python is a programming language.\"\n",
    "text2 = \"I am going to the moon.\"\n",
    "\n",
    "# Get embeddings\n",
    "text1_embedding = get_embedding(text1)\n",
    "text2_embedding = get_embedding(text2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def cosine_sim(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_sim(text1_embedding, text2_embedding)\n",
    "print(f\"Cosine similarity between '{text1}' and '{text2}': {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06447508325343351"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## another example\n",
    "text3 = \"I am sitting on the river bank.\"\n",
    "\n",
    "text3_embedding = get_embedding(text3)\n",
    "\n",
    "cosine_sim(text1_embedding, text3_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2691220067267417"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## another example\n",
    "text4 = \"I am going to the bank.\"\n",
    "\n",
    "text4_embedding = get_embedding(text4)\n",
    "\n",
    "cosine_sim(text3_embedding, text4_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
