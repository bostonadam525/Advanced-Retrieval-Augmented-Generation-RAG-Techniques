# RAG Metrics for Evaluation - Overview, Resources, and Experiments
* By Adam Lang


# Testing LLM Outputs
* These are a general list of testing frameworks that can be used to automate and manually test LLM outputs.
1. BERTScore
   * Uses BERT based models and contextual embeddings.
   * Cosine Similarity to evalute semantic similarity of outputs.
   * Token matching for:
      * Precision
      * Recall
      * F1 score
   * Importance weighting
   * Baseline rescaling
   * Resources for BERTScore
     * [Simple Examples](https://medium.com/@abonia/bertscore-explained-in-5-minutes-0b98553bfb71)
3. BLEU Score
4. RAGAS
5. DeepEval
6. AWS Automated Evaluation
