{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/cookbook/blob/main/gen-ai/openai/agents-sdk-intro.ipynb)\n"
      ],
      "metadata": {
        "id": "d1DGYFfoKvsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI's Agents SDK\n",
        "* This notebook was forked from the James Briggs tutorial on this SDK published on March 12, 2025.\n",
        "* Today's date: 4/11/2025.\n",
        "* I added a few notes and details from James tutorial."
      ],
      "metadata": {
        "id": "sG6pIZ-_7uRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI have released an **Agents SDK**, their version of an open source agent development library.\n",
        "\n",
        "OpenAI have outlined a few features of the library:\n",
        "\n",
        "```\n",
        "* Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\n",
        "* Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\n",
        "* Handoffs: A powerful feature to coordinate and delegate between multiple agents.\n",
        "* Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\n",
        "* Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\n",
        "* Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\n",
        "```\n",
        "\n",
        "([source](https://openai.github.io/openai-agents-python/))\n",
        "\n",
        "We'll focus on covering the essentials here - including the **agent loop**, **python-first**, **guardrails**, and **function tools** features.\n",
        "\n",
        "Let's start by installing the library:"
      ],
      "metadata": {
        "id": "rN2gNfry7tDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3aqSlMc27noo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d93186-8125-420a-f840-5b93b2b2fc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m71.7/75.5 kB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai-agents==0.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's set our [OpenAI API key](https://platform.openai.com/settings/organization/api-keys)."
      ],
      "metadata": {
        "id": "nBGHkh7nBVHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "  getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "Kwpk_H5_BnAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c69607a-7098-4e0d-c1e1-df15dc9519dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init Simple Agent"
      ],
      "metadata": {
        "id": "32tyRUObfw3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You're a helpful assistant\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")"
      ],
      "metadata": {
        "id": "lqkaleTAAjHK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running our Agent"
      ],
      "metadata": {
        "id": "0-zOBFhAEROm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI gives us three methods for running our agent, all via a `Runner` class — those methods are:\n",
        "\n",
        "1. `Runner.run()` which runs in async without streaming.\n",
        "2. `Runner.run_sync()` which runs in sync.\n",
        "3. `Runner.run_streamed()` which runs in async _and_ streams the response back to us --> this is used for streaming.\n",
        "\n",
        "We'll quicky test method **(1)**:"
      ],
      "metadata": {
        "id": "dWO4ozC8ETNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## test the aysnc run method\n",
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "Bjn-gyR9BIn-",
        "outputId": "b2180361-4015-4001-d34c-691adaf03693"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Once upon a time in a quaint village nestled between emerald hills, there lived a young girl named Elara who loved to paint. Every morning, she would venture into the meadows, her easel in tow, capturing the colors of the flowers and the vibrant skies. However, there was one thing Elara longed to paint but had never seen: the legendary Rainbow Falls said to be hidden deep in the Enchanted Forest.\\n\\nOne bright day, inspired by her dreams, Elara decided to embark on an adventure to find the falls. She packed her paints and set off, her heart racing with excitement. The forest was alive with the sound of rustling leaves and distant bird songs. After wandering for hours, she stumbled upon a clearing where sunlight danced through the trees, illuminating a path adorned with wildflowers.\\n\\nFollowing the path, Elara finally arrived at Rainbow Falls. As the sunlight hit the cascading water, it splintered into a myriad of colors, creating a breathtaking rainbow that arched over the falls. Elara set up her easel, her heart swelling with joy as she painted the scene before her.\\n\\nAs she painted, she realized that the falls weren't just a sight to behold; they also sang a melody of peace and happiness. With every stroke of her brush, she felt connected to the beauty of the world around her. \\n\\nWhen she finished, Elara stepped back and gazed at her work, her heart full. She knew this painting would remind her of the magic hidden in chasing one’s dreams.\\n\\nReturning to her village, she displayed her masterpiece in the town square, where it inspired everyone to seek their own adventures. And from that day on, the legend of Rainbow Falls spread, igniting the hearts of dreamers everywhere, proving that sometimes, the journey to find beauty is just as important as the beauty itself.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "1. In most scenarios we'll likely want to be using method **(3)**, ie running async and streaming tokens.\n",
        "2. To do this we need to write a little more code to handle the async streaming and print the tokens as they're returned.\n",
        "3. In most cases you want to stream tokens as soon as you get them as outputs from the LLM.\n",
        "\n",
        "First, we create a `RunResultStreaming` object by calling `Runner.run_streamed(...)`, we then _asynchronously_ iterate through the streamed events returned by our LLM using the `response.stream_events()` method:"
      ],
      "metadata": {
        "id": "W5_yue8fE4eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## streaming example\n",
        "## printing every single event that is returned\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"hello there\"\n",
        ")\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBnhUepDBPHM",
        "outputId": "99c9de14-47f9-4f83-cd62-5a81760888ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_67f8f6357ec4819296844b7378c3d93b0470f88b55d46d6d', created_at=1744369205.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_67f8f6357ec4819296844b7378c3d93b0470f88b55d46d6d', created_at=1744369205.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Hello', item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='!', item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' How', item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' can', item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I', item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' assist', item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' you', item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' today', item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='?', item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDoneEvent(content_index=0, item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, text='Hello! How can I assist you today?', type='response.output_text.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', output_index=0, part=ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_67f8f6357ec4819296844b7378c3d93b0470f88b55d46d6d', created_at=1744369205.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=27), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='msg_67f8f635d6d481928441042074bc3ebe0470f88b55d46d6d', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "* We can now see EVERY SINGLE EVENT that is streamed from the OpenAI API call."
      ],
      "metadata": {
        "id": "ZrCECuprggO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We can filter these various event types to find only raw tokens\n",
        "* Note: this only works for a direct LLM output, once you introduce LLM tools it gets more complicated..."
      ],
      "metadata": {
        "id": "iH-RoQfzMj6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "# we do need to reinitialize our runner before re-executing\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\" and \\\n",
        "        isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGQANyvXG9zJ",
        "outputId": "d5c8ea6f-5bdb-42b5-d44a-4c57c351e764"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a quaint village nestled between rolling hills, there lived a curious little girl named Lila. Every day after school, she explored the surrounding woods, collecting colorful leaves and smooth stones, all the while dreaming of adventures beyond her village.\n",
            "\n",
            "One sunny afternoon, while wandering deeper into the forest than ever before, she stumbled upon a shimmering pond. The water was so clear that it reflected the sky like a mirror. Sitting by the edge, Lila noticed a tiny, golden fish struggling to escape a patch of tangled weeds.\n",
            "\n",
            "Without hesitation, she plunged her hands into the cool water and gently freed the fish. To her astonishment, the fish began to glow and transformed into a beautiful fairy. With a twinkle in her eye, the fairy thanked Lila and granted her one wish.\n",
            "\n",
            "Lila thought carefully. Instead of wishing for toys or riches, she said, \"I wish for more adventures for everyone in my village.\" The fairy smiled, then waved her wand. \n",
            "\n",
            "From that day on, the village was filled with magic. Mysterious trails in the woods led to hidden treasures, and the fields burst with flowers that told stories when the wind blew. Every villager found a new adventure awaiting them each day.\n",
            "\n",
            "And Lila? She became a beloved storyteller, sharing the tales of wonder that changed her village forever, reminding everyone that sometimes, the most magical adventures start with a single act of kindness."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "9Iixlv0sRMqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* OpenAI included **function tools** as a **key feature** in their Agents SDK announcement.\n",
        "* After turning everyone away from using `_function calling_` to instead use `_tool calling_`, OpenAI have now decided that an LLM deciding to execute some code will be called _\"function tools\"_.\n",
        "\n",
        "* To use `_function tools_` in Agents SDK we simply decorate a function with the `@function_tool` decorator like so:"
      ],
      "metadata": {
        "id": "kDHORYUcROZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import function_tool\n",
        "\n",
        "## multiplication tool\n",
        "@function_tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiplies `x` and `y` to provide a precise\n",
        "    answer.\"\"\"\n",
        "    return x*y"
      ],
      "metadata": {
        "id": "tJz7BlwiNOa3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have:\n",
        "1. taken extra care to include a clear and descriptive function name\n",
        "2. relatively clear parameter names\n",
        "3. type annotations for both input parameters and expected output, and a\n",
        "\n",
        "4. natural language docstring that will be fed to the LLM and explain to it `_what_` this tool does.\n",
        "\n",
        "To run our agent `_with_ tools` we simply pass our new tool into the `tools` parameter during `Agent` initialization."
      ],
      "metadata": {
        "id": "I8b8KektTQbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    ## system prompt/instructions\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=[multiply]  # note that we expect a list of tools\n",
        ")"
      ],
      "metadata": {
        "id": "WnT4C9WBTO_M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's initialize a new runner and execute our agent with tools:"
      ],
      "metadata": {
        "id": "VtioNxflUS-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSOREQ-XUSVi",
        "outputId": "58fa5584-a13d-426f-c7fc-38fdfb86c2ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7de2fe37f7e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_67f8f77187ec8192be81a2db9d6deb5d06fb1a6a4abcfcda', created_at=1744369521.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_67f8f77187ec8192be81a2db9d6deb5d06fb1a6a4abcfcda', created_at=1744369521.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='', call_id='call_tqiLuOqYNoeVRALK3Kfs04cb', name='multiply', type='function_call', id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', status='in_progress'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{\"', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='x', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='7', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='.', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='814', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta=',\"', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='y', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='103', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='.', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='892', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='}', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDoneEvent(arguments='{\"x\":7.814,\"y\":103.892}', item_id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', output_index=0, type='response.function_call_arguments.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_tqiLuOqYNoeVRALK3Kfs04cb', name='multiply', type='function_call', id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', status='completed'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_67f8f77187ec8192be81a2db9d6deb5d06fb1a6a4abcfcda', created_at=1744369521.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_tqiLuOqYNoeVRALK3Kfs04cb', name='multiply', type='function_call', id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=113, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=22, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=135), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7de2fe37f7e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_tqiLuOqYNoeVRALK3Kfs04cb', name='multiply', type='function_call', id='fc_67f8f772876c8192af6ea47a95435cfd06fb1a6a4abcfcda', status='completed'), type='tool_call_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7de2fe37f7e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item={'call_id': 'call_tqiLuOqYNoeVRALK3Kfs04cb', 'output': '811.812088', 'type': 'function_call_output'}, output='811.812088', type='tool_call_output_item'), type='run_item_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_67f8f773318c8192a582619446f12e4f06fb1a6a4abcfcda', created_at=1744369523.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_67f8f773318c8192a582619446f12e4f06fb1a6a4abcfcda', created_at=1744369523.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='The', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' result', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' of', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' multiplying', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\(', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='7', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='814', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=')', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' by', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\(', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='103', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='892', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=')', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' is', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' approximately', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\(', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='811', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='812', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' \\\\', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=').', item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDoneEvent(content_index=0, item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is approximately \\\\( 811.812 \\\\).', type='response.output_text.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', output_index=0, part=ResponseOutputText(annotations=[], text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is approximately \\\\( 811.812 \\\\).', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', content=[ResponseOutputText(annotations=[], text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is approximately \\\\( 811.812 \\\\).', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_67f8f773318c8192a582619446f12e4f06fb1a6a4abcfcda', created_at=1744369523.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', content=[ResponseOutputText(annotations=[], text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is approximately \\\\( 811.812 \\\\).', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=146, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=30, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=176), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7de2fe37f7e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='msg_67f8f773bd70819295a59db5172767ae06fb1a6a4abcfcda', content=[ResponseOutputText(annotations=[], text='The result of multiplying \\\\( 7.814 \\\\) by \\\\( 103.892 \\\\) is approximately \\\\( 811.812 \\\\).', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look closely at the fourth event object we will see `ResponseFunctionToolCall`, meaning our `multiply` tool was called by our LLM. Following this event object we can also see several events containing the `ResponseFunctionCallArgumentsDeltaEvent` type inside the `data` field — these are the input parameters for our tool.\n",
        "\n",
        "Let's rerun that but this time we will process the event outputs to generate a cleaner and more readable output."
      ],
      "metadata": {
        "id": "jQZu6iyPU0wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.types.responses import (\n",
        "    ResponseFunctionCallArgumentsDeltaEvent,  # tool call streaming\n",
        "    ResponseCreatedEvent,  # start of new event like tool call or final answer\n",
        ")\n",
        "\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "## tokens as streamed by LLM\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\":\n",
        "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
        "            # this is streamed parameters for our tool call\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
        "            # this is streamed final answer tokens\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        # this tells us which agent is currently in use\n",
        "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
        "    elif event.type == \"run_item_stream_event\": ## includes tool calling\n",
        "        # these are events containing info that we'd typically\n",
        "        # stream out to a user or some downstream process\n",
        "        if event.name == \"tool_called\":\n",
        "            # this is the collection of our _full_ tool call after our tool\n",
        "            # tokens have all been streamed\n",
        "            print()\n",
        "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
        "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
        "        elif event.name == \"tool_output\":\n",
        "            # this is the response from our tool execution\n",
        "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhFvNVowUrd5",
        "outputId": "aacd56c4-4ee3-4480-f3be-515945513dac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Current Agent: Assistant\n",
            "{\"x\":7.814,\"y\":103.892}\n",
            "> Tool Called, name: multiply\n",
            "> Tool Called, args: {\"x\":7.814,\"y\":103.892}\n",
            "> Tool Output: 811.812088\n",
            "The result of multiplying 7.814 by 103.892 is approximately 811.812."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardrails"
      ],
      "metadata": {
        "id": "CqqWET4O93il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* OpenAI have also included guardrails in the Agents SDK.\n",
        "* These come as:\n",
        "  1. `_input guardrails_`\n",
        "    * checks that the input going into your LLM is \"safe\"\n",
        "  2. `_output guardrails_`\n",
        "    * checks that the output from your LLM is \"safe\".\n",
        "\n",
        "\n",
        "Let's see how to use them. First, we'll implement a guardrail powered by another LLM (more tokens means more $$$ for OpenAI)."
      ],
      "metadata": {
        "id": "CBrf2Ao396Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "# define structure of output for any guardrail agents\n",
        "# thus we are checking the output via the guardrail\n",
        "# forces a STRUCTURED OUTPUT from the agent\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_triggered: bool\n",
        "    reasoning: str\n",
        "\n",
        "# define another agent that\n",
        "# checks if user is asking about political opinions -- we dont want it to do this\n",
        "politics_agent = Agent(\n",
        "    name=\"Politics check\",\n",
        "    instructions=\"Check if the user is asking you about political opinions\",\n",
        "    output_type=GuardrailOutput,\n",
        ")"
      ],
      "metadata": {
        "id": "I1LT_UpiXFg0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can call this agent directly:"
      ],
      "metadata": {
        "id": "vRRFcCFGBHSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what do you think about President Donald Trump?\"\n",
        "\n",
        "result = await Runner.run(starting_agent=politics_agent, input=query)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4kgYMC9A7WP",
        "outputId": "97425fca-a3fe-4441-b197-d635570a27bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunResult(input='what do you think about President Donald Trump?', new_items=[MessageOutputItem(agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None), raw_item=ResponseOutputMessage(id='msg_67f8f8fe798c8192938774082c4292a20e3b6637919fb604', content=[ResponseOutputText(annotations=[], text='{\"is_triggered\":true,\"reasoning\":\"The user is directly asking for an opinion about a political figure, which involves political opinions.\"}', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item')], raw_responses=[ModelResponse(output=[ResponseOutputMessage(id='msg_67f8f8fe798c8192938774082c4292a20e3b6637919fb604', content=[ResponseOutputText(annotations=[], text='{\"is_triggered\":true,\"reasoning\":\"The user is directly asking for an opinion about a political figure, which involves political opinions.\"}', type='output_text')], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=30, output_tokens=30, total_tokens=60), referenceable_id='resp_67f8f8fdb13481928e5cc1e8f1a19d9e0e3b6637919fb604')], final_output=GuardrailOutput(is_triggered=True, reasoning='The user is directly asking for an opinion about a political figure, which involves political opinions.'), input_guardrail_results=[], output_guardrail_results=[], _last_agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output from our agent is hidden away in there, we extract it like so:"
      ],
      "metadata": {
        "id": "6NfFXKYOBeJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrS4dT5mC7jN",
        "outputId": "9a7e0865-cefa-4889-bd69-2724e6a051de"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GuardrailOutput(is_triggered=True, reasoning='The user is directly asking for an opinion about a political figure, which involves political opinions.')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "* Aha! The Guardrail caught that we were asking a political question and it blocked it."
      ],
      "metadata": {
        "id": "GKUM7t8kjPPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To integrate this with our other agents we need to move our logic into a **single function decorated with the `@input_guardrail` decorator.**\n",
        "\n",
        "When defining these guardrails we need to follow the following structure:\n",
        "\n",
        "* Input parameters must include a `ctx` (context), `agent`, and `input` (the user's query in this case). Note that below we will only use the `input` parameter.\n",
        "* Output must be a `GuardrailFunctionOutput` object."
      ],
      "metadata": {
        "id": "okpF-4pAC_zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import (\n",
        "    GuardrailFunctionOutput,\n",
        "    RunContextWrapper,\n",
        "    input_guardrail\n",
        ")\n",
        "\n",
        "# this is the guardrail function that returns GuardrailFunctionOutput object\n",
        "@input_guardrail\n",
        "async def politics_guardrail(\n",
        "    ctx: RunContextWrapper[None], #not using but is required\n",
        "    agent: Agent, # not using but is required\n",
        "    input: str,\n",
        ") -> GuardrailFunctionOutput:\n",
        "    # run agent to check if guardrail is triggered\n",
        "    response = await Runner.run(starting_agent=politics_agent, input=input)\n",
        "    # format response into GuardrailFunctionOutput\n",
        "    return GuardrailFunctionOutput(\n",
        "        output_info=response.final_output,\n",
        "        tripwire_triggered=response.final_output.is_triggered,\n",
        "    )"
      ],
      "metadata": {
        "id": "o3UQIt-qBLTK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can initialize our normal agent with the `input_guardrails` parameter:"
      ],
      "metadata": {
        "id": "N8eCZluTFnYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=[multiply],\n",
        "    input_guardrails=[politics_guardrail],  # note this is a list of guardrails\n",
        ")"
      ],
      "metadata": {
        "id": "_JYELjOSFoED"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run it! We'll stick with `Runner.run` for the sake of brevity:"
      ],
      "metadata": {
        "id": "oxYq9Aq0GIv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GKDUcBYMGGcG",
        "outputId": "d9c7c7f5-499c-4c03-bbc0-36f005132288"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The result of \\\\( 7.814 \\\\) multiplied by \\\\( 103.892 \\\\) is \\\\( 811.812088 \\\\).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if our guardrail will trigger:"
      ],
      "metadata": {
        "id": "c5sN23B_HcPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what do you think about President Donald Trump?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "-tncyfxYGdRn",
        "outputId": "8432632a-44c5-44f0-d352-3d9841fb03ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InputGuardrailTripwireTriggered",
          "evalue": "Guardrail InputGuardrail triggered tripwire",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInputGuardrailTripwireTriggered\u001b[0m           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f7ec17748087>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = await Runner.run(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstarting_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"what do you think about President Donald Trump?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_turn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                         input_guardrail_results, turn_result = await asyncio.gather(\n\u001b[0m\u001b[1;32m    211\u001b[0m                             cls._run_input_guardrails(\n\u001b[1;32m    212\u001b[0m                                 \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36m_run_input_guardrails\u001b[0;34m(cls, agent, guardrails, input, context)\u001b[0m\n\u001b[1;32m    803\u001b[0m                     )\n\u001b[1;32m    804\u001b[0m                 )\n\u001b[0;32m--> 805\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInputGuardrailTripwireTriggered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0mguardrail_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInputGuardrailTripwireTriggered\u001b[0m: Guardrail InputGuardrail triggered tripwire"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "* We expected the error above!\n",
        "* Great, our guardrail triggered! The `output_guardrail` type is implemented in almost the exact same way, but uses the `@output_guardrail` decorator when defining the guardrail function, and the `output_guardrails` parameter when defining our `Agent`."
      ],
      "metadata": {
        "id": "lvGzaTDJHmts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational Agents"
      ],
      "metadata": {
        "id": "nnVc_II-IcRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So far we've only seen how to use our agents with single messages.\n",
        "* Many use-cases require chat history to make our agents conversational. To implement that we simply provide a list of messages to our `Runner`.\n",
        "\n",
        "Let's see how this works, first we send a single message:"
      ],
      "metadata": {
        "id": "2EV4EtGZIevd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"remember the number 7.814 for me please\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "wK558-qII5WA",
        "outputId": "f3208698-ebd5-4bf5-d281-f4443db4bf21"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I can't remember things for you permanently. However, you can save that number in your notes or a document for easy access later! If you'd like, I can help you with something else.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, we can help our agent remember this information. We can use the `.to_input_list()` method to format our `result` into a list of messages for our next query."
      ],
      "metadata": {
        "id": "fD_kDKxFI5LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_input_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs-buAAvJh8F",
        "outputId": "ebcfa8e1-0372-4843-efa0-0866378d6ae0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': 'remember the number 7.814 for me please', 'role': 'user'},\n",
              " {'id': 'msg_67f8fa9c8b148192bdd73e30191260690de123c94b288fe1',\n",
              "  'content': [{'annotations': [],\n",
              "    'text': \"I can't remember things for you permanently. However, you can save that number in your notes or a document for easy access later! If you'd like, I can help you with something else.\",\n",
              "    'type': 'output_text'}],\n",
              "  'role': 'assistant',\n",
              "  'status': 'completed',\n",
              "  'type': 'message'}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We merge this with our next message:"
      ],
      "metadata": {
        "id": "Ia1n7UG_JkYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=result.to_input_list() + [\n",
        "        {\"role\": \"user\", \"content\": \"multiply the last number by 103.892\"}\n",
        "    ]\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y4WKSTObJokR",
        "outputId": "578cea80-0f45-4d1a-e420-322feb8a3d2b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The result of multiplying 7.814 by 103.892 is approximately 811.812.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kZHZne6WbkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our agent can remember our previous interactions after all!"
      ],
      "metadata": {
        "id": "7bGdZMCLKDMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "khGNoTd_KH4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is our rapid-fire overview of OpenAI's new Agents SDK. We've covered most of the essentials here but there are many other features in the library, and many of the features we included here come with plenty of different ways to use. The SDK is already fairly substantial and certainly worth keeping an eye on."
      ],
      "metadata": {
        "id": "aFi__V9hKIKQ"
      }
    }
  ]
}