{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loaders in LlamaIndex\n",
        "* Notebook by Adam Lang\n",
        "* Date: 3/11/2024"
      ],
      "metadata": {
        "id": "qN9QSArAKKXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are Data Loaders?\n",
        "* Read and load various data sources and types.\n",
        "* Facilitates conversion of different data types into a **document** format that is readable and usable by llamaindex.\n",
        "* LlamaIndex supports 100+ Data Loaders within the Llama Hub."
      ],
      "metadata": {
        "id": "RSVLAbGfKQaF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A516HouFKGuC",
        "outputId": "f9e399a6-ed4a-45bb-93ca-dea35828ca1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.18-py3-none-any.whl (5.6 kB)\n",
            "Collecting llama-index-agent-openai<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.1.5-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.8-py3-none-any.whl (25 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.18 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.18.post1-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.6-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.3-py3-none-any.whl (6.6 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.7-py3-none-any.whl (9.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.9-py3-none-any.whl (35 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.3-py3-none-any.whl (2.5 kB)\n",
            "Collecting llama-index-vector-stores-chroma<0.2.0,>=0.1.1 (from llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading llama_index_vector_stores_chroma-0.1.6-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (3.9.3)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.18->llama-index) (4.10.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading PyMuPDF-1.23.26-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse<0.4.0,>=0.3.3 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.3.8-py3-none-any.whl (6.7 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.18->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.18->llama-index) (1.14.1)\n",
            "Collecting chromadb<0.5.0,>=0.4.22 (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.18->llama-index) (2.6.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.18->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.18->llama-index) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.18->llama-index) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.18->llama-index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.18->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.18->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.18->llama-index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.18->llama-index) (1.7.0)\n",
            "Collecting PyMuPDFb==1.23.22 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.18->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.18->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.18->llama-index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.18->llama-index)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.18->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.18->llama-index) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.18->llama-index) (1.2.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.1.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading uvicorn-0.28.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m433.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_api-1.23.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_sdk-1.23.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (6.1.3)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.62.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.18->llama-index) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.18->llama-index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.18->llama-index) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.18->llama-index) (1.16.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.0.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.2.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (24.3.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.12)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.62.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_proto-1.23.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (67.7.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.20.3)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.13.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=2d6ac4b421d51eb392b5def03743273c6ddc17323d2485861b231b4b78f87f6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: striprtf, pypika, monotonic, mmh3, dirtyjson, websockets, uvloop, python-dotenv, pypdf, PyMuPDFb, pulsar-client, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, pymupdf, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, httpcore, coloredlogs, bs4, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, openai, llamaindex-py-client, opentelemetry-instrumentation-fastapi, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, chromadb, llama-index-vector-stores-chroma, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-agent-openai, llama-index-program-openai, llama-index-cli, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.0.2\n",
            "    Uninstalling importlib_metadata-7.0.2:\n",
            "      Successfully uninstalled importlib_metadata-7.0.2\n",
            "Successfully installed PyMuPDFb-1.23.22 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 bs4-0.0.2 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 fastapi-0.110.0 h11-0.14.0 httpcore-1.0.4 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 importlib-metadata-6.11.0 kubernetes-29.0.0 llama-index-0.10.18 llama-index-agent-openai-0.1.5 llama-index-cli-0.1.8 llama-index-core-0.10.18.post1 llama-index-embeddings-openai-0.1.6 llama-index-indices-managed-llama-cloud-0.1.3 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.7 llama-index-multi-modal-llms-openai-0.1.4 llama-index-program-openai-0.1.4 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.9 llama-index-readers-llama-parse-0.1.3 llama-index-vector-stores-chroma-0.1.6 llama-parse-0.3.8 llamaindex-py-client-0.1.13 marshmallow-3.21.1 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.17.1 openai-1.13.3 opentelemetry-api-1.23.0 opentelemetry-exporter-otlp-proto-common-1.23.0 opentelemetry-exporter-otlp-proto-grpc-1.23.0 opentelemetry-instrumentation-0.44b0 opentelemetry-instrumentation-asgi-0.44b0 opentelemetry-instrumentation-fastapi-0.44b0 opentelemetry-proto-1.23.0 opentelemetry-sdk-1.23.0 opentelemetry-semantic-conventions-0.44b0 opentelemetry-util-http-0.44b0 orjson-3.9.15 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.4.0 pymupdf-1.23.26 pypdf-4.1.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.36.3 striprtf-0.0.26 tiktoken-0.6.0 typing-inspect-0.9.0 uvicorn-0.28.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "# install library\n",
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load a PDF file\n",
        "* download the data"
      ],
      "metadata": {
        "id": "xXyrlXItK8Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget 'https://raw.githubusercontent.com/aravindpai/Speech-Recognition/1882379d3152c8cd830d74e677be4dd161d024ea/transformers.pdf' -O 'data/transformers.pdf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMuiPQvoK3Tw",
        "outputId": "9d54d5f0-6ca7-4be3-be06-0507689ba48d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-11 20:21:07--  https://raw.githubusercontent.com/aravindpai/Speech-Recognition/1882379d3152c8cd830d74e677be4dd161d024ea/transformers.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2215244 (2.1M) [application/octet-stream]\n",
            "Saving to: ‘data/transformers.pdf’\n",
            "\n",
            "data/transformers.p 100%[===================>]   2.11M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-03-11 20:21:08 (33.1 MB/s) - ‘data/transformers.pdf’ saved [2215244/2215244]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using PDFReader in LlamaIndex\n",
        "    * documentation from LlamaIndex: https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html"
      ],
      "metadata": {
        "id": "1TNavy6uLXeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from llama_index.core import download_loader"
      ],
      "metadata": {
        "id": "Vzk75rWCLRFg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDFreader from llamaindex\n",
        "PDFReader = download_loader(\"PDFReader\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck5ouF6PLfMV",
        "outputId": "a7d5fce4-8f30-4020-f62e-7814067622ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-877e68370f9b>:2: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
            "  PDFReader = download_loader(\"PDFReader\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PDFReader()"
      ],
      "metadata": {
        "id": "5MNH2VRpMgjC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load_data(file=Path('./data/transformers.pdf'))"
      ],
      "metadata": {
        "id": "YDcAZE9-MlWW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check len of documents\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YBzUX8JMrMw",
        "outputId": "61b48b63-3568-4275-a25e-9cb01ecc47d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print first line of text\n",
        "documents[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "is3o4OJqMuzL",
        "outputId": "b86a596b-6de3-4bce-c3e7-c78ffbaec1d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "summary:\n",
        "* Above we can see the process to load a PDF file in LlamaIndex."
      ],
      "metadata": {
        "id": "Wper9qy1M9O_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Loading CSV files"
      ],
      "metadata": {
        "id": "zmqLd8vKNBku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://datahack-prod.s3.amazonaws.com/train_file/train_v9rqX0R.csv -O 'data/transactions.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjoFH3HhMxwV",
        "outputId": "cbbc71a8-5ee0-4f08-c2b8-0222cb2891de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-11 20:29:23--  https://datahack-prod.s3.amazonaws.com/train_file/train_v9rqX0R.csv\n",
            "Resolving datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)... 16.12.40.67, 52.219.62.84, 52.219.160.47, ...\n",
            "Connecting to datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)|16.12.40.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 869537 (849K) [text/csv]\n",
            "Saving to: ‘data/transactions.csv’\n",
            "\n",
            "data/transactions.c 100%[===================>] 849.16K   718KB/s    in 1.2s    \n",
            "\n",
            "2024-03-11 20:29:26 (718 KB/s) - ‘data/transactions.csv’ saved [869537/869537]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "reader = SimpleDirectoryReader(input_files=['/content/data/transactions.csv'])\n"
      ],
      "metadata": {
        "id": "m29ASxLhN3Dv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instatiate document loader in documents variable\n",
        "documents = reader.load_data()"
      ],
      "metadata": {
        "id": "SOyzofpFNScd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print documents first line\n",
        "print(documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRXc85AgPI6Z",
        "outputId": "1ee0ca15-a032-4ec9-c6e7-75cd84cd3275"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc ID: 7728be25-1376-4692-8d5f-eaab7c8be713\n",
            "Text: FDA15, 9.3, Low Fat, 0.016047301, Dairy, 249.8092, OUT049, 1999,\n",
            "Medium, Tier 1, Supermarket Type1, 3735.138 DRC01, 5.92, Regular,\n",
            "0.019278216, Soft Drinks, 48.2692, OUT018, 2009, Medium, Tier 3,\n",
            "Supermarket Type2, 443.4228 FDN15, 17.5, Low Fat, 0.016760075, Meat,\n",
            "141.618, OUT049, 1999, Medium, Tier 1, Supermarket Type1, 2097.27\n",
            "FDX07, 19.2, Reg...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Loading a Web Page"
      ],
      "metadata": {
        "id": "7UMtDRBMPmf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import download_loader\n",
        "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7l4kMO8PW82",
        "outputId": "a44faee6-33de-4e84-e6b4-1ccee6840ae6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-ef4a0bac76ba>:2: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
            "  SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = SimpleWebPageReader()"
      ],
      "metadata": {
        "id": "IWZxEkO-P6nK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load_data(urls=['https://huggingface.co/blog/moe'])"
      ],
      "metadata": {
        "id": "YB4eZB7UQSK1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "tBQhHtiDQXjG",
        "outputId": "013d2582-3ed8-4f34-ebfd-e5361b298f09"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!doctype html>\\n<html class=\"\">\\n\\t<head>\\n\\t\\t<meta charset=\"utf-8\" />\\n\\t\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=no\" />\\n\\t\\t<meta name=\"description\" content=\"We’re on a journey to advance and democratize artificial intelligence through open source and open science.\" />\\n\\t\\t<meta property=\"fb:app_id\" content=\"1321688464574422\" />\\n\\t\\t<meta name=\"twitter:card\" content=\"summary_large_image\" />\\n\\t\\t<meta name=\"twitter:site\" content=\"@huggingface\" />\\n\\t\\t<meta property=\"og:title\" content=\"Mixture of Experts Explained\" />\\n\\t\\t<meta property=\"og:type\" content=\"website\" />\\n\\t\\t<meta property=\"og:url\" content=\"https://huggingface.co/blog/moe\" />\\n\\t\\t<meta property=\"og:image\" content=\"https://huggingface.co/blog/assets/moe/thumbnail.png\" />\\n\\n\\t\\t<link rel=\"stylesheet\" href=\"/front/build/kube-78db83e/style.css\" />\\n\\n\\t\\t<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" />\\n\\t\\t<link\\n\\t\\t\\thref=\"https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap\"\\n\\t\\t\\trel=\"stylesheet\"\\n\\t\\t/>\\n\\t\\t<link\\n\\t\\t\\thref=\"https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap\"\\n\\t\\t\\trel=\"stylesheet\"\\n\\t\\t/>\\n\\n\\t\\t<link\\n\\t\\t\\trel=\"preload\"\\n\\t\\t\\thref=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css\"\\n\\t\\t\\tas=\"style\"\\n\\t\\t\\tonload=\"this.onload=null;this.rel=\\'stylesheet\\'\"\\n\\t\\t/>\\n\\t\\t<noscript>\\n\\t\\t\\t<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css\" />\\n\\t\\t</noscript>\\n\\n\\t\\t<link rel=\"canonical\" href=\"https://huggingface.co/blog/moe\">\\n<link rel=\"alternate\" hreflang=\"zh\" href=\"https://huggingface.co/blog/zh/moe\">  <!-- HEAD_svelte-vwinwk_START --><link rel=\"alternate\" type=\"application/rss+xml\" href=\"/blog/feed.xml\" title=\"Hugging Face Blog\"><!-- HEAD_svelte-vwinwk_END -->\\n\\n\\t\\t<title>Mixture of Experts Explained</title>\\n\\n\\t\\t<script defer data-domain=\"huggingface.co\" src=\"/js/script.js\"></script>\\n\\t\\t<script type=\"text/javascript\" src=\"https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js\" defer></script>\\n\\t</head>\\n\\t<body class=\"flex flex-col min-h-screen bg-white dark:bg-gray-950 text-black BlogPage\">\\n\\t\\t\\n\\n<div class=\"flex min-h-screen flex-col\">\\n\\t<div class=\"SVELTE_HYDRATER contents\" data-props=\"{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false}\" data-target=\"MainHeader\"><header class=\"border-b border-gray-100 \"><div class=\"w-full px-4 container flex h-16 items-center\"><div class=\"flex flex-1 items-center\"><a class=\"mr-5 flex flex-none items-center lg:mr-6\" href=\"/\"><img alt=\"Hugging Face\\'s logo\" class=\"w-7 md:mr-2\" src=\"/front/assets/huggingface_logo-noborder.svg\">\\n\\t\\t\\t\\t<span class=\"hidden whitespace-nowrap text-lg font-bold md:block\">Hugging Face</span></a>\\n\\t\\t\\t<div class=\"relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6\"><input autocomplete=\"off\" class=\"w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl \" name=\"\" placeholder=\"Search models, datasets, users...\"  spellcheck=\"false\" type=\"text\" value=\"\">\\n\\t<svg class=\"absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z\" fill=\"currentColor\"></path></svg>\\n\\t</div>\\n\\t\\t\\t<div class=\"flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden\"><button class=\"relative z-40 flex h-6 w-8 items-center justify-center\" type=\"button\"><svg width=\"1em\" height=\"1em\" viewBox=\"0 0 10 10\" class=\"text-xl\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" preserveAspectRatio=\"xMidYMid meet\" fill=\"currentColor\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z\"></path></svg>\\n\\t\\t</button>\\n\\n\\t</div></div>\\n\\t\\t<nav aria-label=\"Main\" class=\"ml-auto hidden lg:block\"><ul class=\"flex items-center space-x-1.5 xl:space-x-2\"><li><a class=\"group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-indigo-700\" href=\"/models\"><svg class=\"mr-1.5 text-gray-400 group-hover:text-indigo-500\" style=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path class=\"uim-quaternary\" d=\"M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z\" opacity=\".25\" fill=\"currentColor\"></path><path class=\"uim-tertiary\" d=\"M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z\" opacity=\".5\" fill=\"currentColor\"></path><path class=\"uim-primary\" d=\"M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z\" fill=\"currentColor\"></path></svg>\\n\\t\\t\\t\\t\\tModels</a>\\n\\t\\t\\t</li><li><a class=\"group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-red-700\" href=\"/datasets\"><svg class=\"mr-1.5 text-gray-400 group-hover:text-red-500\" style=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 25 25\"><ellipse cx=\"12.5\" cy=\"5\" fill=\"currentColor\" fill-opacity=\"0.25\" rx=\"7.5\" ry=\"2\"></ellipse><path d=\"M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z\" fill=\"currentColor\" opacity=\"0.5\"></path><path d=\"M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z\" fill=\"currentColor\" opacity=\"0.5\"></path><path d=\"M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z\" fill=\"currentColor\"></path></svg>\\n\\t\\t\\t\\t\\tDatasets</a>\\n\\t\\t\\t</li><li><a class=\"group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-blue-700\" href=\"/spaces\"><svg class=\"mr-1.5 text-gray-400 group-hover:text-blue-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" viewBox=\"0 0 25 25\"><path opacity=\".5\" d=\"M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z\" fill=\"currentColor\"></path><path opacity=\".75\" fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z\" fill=\"currentColor\"></path><path opacity=\".25\" d=\"M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z\" fill=\"currentColor\"></path></svg>\\n\\t\\t\\t\\t\\tSpaces</a>\\n\\t\\t\\t</li><li><a class=\"group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700\" href=\"/posts\"><svg class=\"mr-1.5 text-gray-400 group-hover:text-yellow-500 !text-yellow-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" viewBox=\"0 0 12 12\" preserveAspectRatio=\"xMidYMid meet\"><path fill=\"currentColor\" fill-rule=\"evenodd\" d=\"M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z\" clip-rule=\"evenodd\"></path></svg>\\n\\t\\t\\t\\t\\tPosts</a>\\n\\t\\t\\t</li><li><a class=\"group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700\" href=\"/docs\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" class=\"mr-1.5 text-gray-400 group-hover:text-yellow-500\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path opacity=\"0.5\" d=\"M20.9022 5.10334L10.8012 10.8791L7.76318 9.11193C8.07741 8.56791 8.5256 8.11332 9.06512 7.7914L15.9336 3.73907C17.0868 3.08811 18.5002 3.26422 19.6534 3.91519L19.3859 3.73911C19.9253 4.06087 20.5879 4.56025 20.9022 5.10334Z\" fill=\"currentColor\"></path><path d=\"M10.7999 10.8792V28.5483C10.2136 28.5475 9.63494 28.4139 9.10745 28.1578C8.5429 27.8312 8.074 27.3621 7.74761 26.7975C7.42122 26.2327 7.24878 25.5923 7.24756 24.9402V10.9908C7.25062 10.3319 7.42358 9.68487 7.74973 9.1123L10.7999 10.8792Z\" fill=\"currentColor\" fill-opacity=\"0.75\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M21.3368 10.8499V6.918C21.3331 6.25959 21.16 5.61234 20.8346 5.03949L10.7971 10.8727L10.8046 10.874L21.3368 10.8499Z\" fill=\"currentColor\"></path><path opacity=\"0.5\" d=\"M21.7937 10.8488L10.7825 10.8741V28.5486L21.7937 28.5234C23.3344 28.5234 24.5835 27.2743 24.5835 25.7335V13.6387C24.5835 12.0979 23.4365 11.1233 21.7937 10.8488Z\" fill=\"currentColor\"></path></svg>\\n\\t\\t\\t\\t\\tDocs</a>\\n\\t\\t\\t</li>\\n\\t\\t<li class=\"max-2xl:hidden\"><div class=\"relative \">\\n\\t<button class=\"px-2 py-0.5 group hover:text-green-700 dark:hover:text-gray-400 flex items-center \" type=\"button\">\\n\\t\\t<svg class=\"mr-1.5 text-gray-400 group-hover:text-green-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path class=\"uim-tertiary\" d=\"M19 6H5a3 3 0 0 0-3 3v2.72L8.837 14h6.326L22 11.72V9a3 3 0 0 0-3-3z\" opacity=\".5\" fill=\"currentColor\"></path><path class=\"uim-primary\" d=\"M10 6V5h4v1h2V5a2.002 2.002 0 0 0-2-2h-4a2.002 2.002 0 0 0-2 2v1h2zm-1.163 8L2 11.72V18a3.003 3.003 0 0 0 3 3h14a3.003 3.003 0 0 0 3-3v-6.28L15.163 14H8.837z\" fill=\"currentColor\"></path></svg>\\n\\t\\t\\tSolutions\\n\\t\\t</button>\\n\\t\\n\\t\\n\\t</div></li>\\n\\t\\t<li><a class=\"group flex items-center px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400\" href=\"/pricing\">Pricing\\n\\t\\t\\t</a></li>\\n\\n\\t\\t<li><div class=\"relative group\">\\n\\t<button class=\"px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center \" type=\"button\">\\n\\t\\t<svg class=\"mr-1.5 text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-400\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" viewBox=\"0 0 32 18\" preserveAspectRatio=\"xMidYMid meet\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z\" fill=\"currentColor\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z\" fill=\"currentColor\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z\" fill=\"currentColor\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z\" fill=\"currentColor\"></path></svg>\\n\\t\\t\\t\\n\\t\\t</button>\\n\\t\\n\\t\\n\\t</div></li>\\n\\t\\t<li><hr class=\"h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800\"></li>\\n\\t\\t<li><a class=\"block cursor-pointer px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400\" href=\"/login\">Log In\\n\\t\\t\\t\\t</a></li>\\n\\t\\t\\t<li><a class=\"rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black\" href=\"/join\">Sign Up\\n\\t\\t\\t\\t\\t</a></li></ul></nav></div></header></div>\\n\\t\\n\\t<div class=\"SVELTE_HYDRATER contents\" data-props=\"{}\" data-target=\"GoogleAnalyticsTracker\"></div>\\n\\t\\n\\t\\n\\t<div class=\"SVELTE_HYDRATER contents\" data-props=\"{}\" data-target=\"SSOBanner\"></div>\\n\\t\\n\\n\\t<main class=\"flex flex-1 flex-col\"><div class=\"blog-content copiable-code-container container prose mx-auto max-w-3xl pb-16 pt-6 lg:prose-lg 2xl:prose-lg lg:pt-16 2xl:max-w-4xl\"><div class=\"SVELTE_HYDRATER contents\" data-props=\"{}\" data-target=\"RepoCodeCopy\"><div></div></div>\\n\\t\\t<div class=\"mb-4\"><a href=\"/blog\" class=\"flex items-center font-sans !text-gray-500 !no-underline hover:!underline\"><svg class=\"mr-2 h-3 w-3\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M14 26l1.41-1.41L7.83 17H28v-2H7.83l7.58-7.59L14 6L4 16l10 10z\" fill=\"currentColor\"></path></svg>\\n\\t\\t\\t\\tBack to blog</a></div>\\n\\t\\t<!-- HTML_TAG_START --><h1 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"mixture-of-experts-explained\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#mixture-of-experts-explained\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tMixture of Experts Explained\\n\\t</span>\\n</h1><div><div class=\"mb-4 flex items-center gap-x-4 text-base\">\\n\\t\\t<span>Published\\n\\t\\t\\t\\tDecember 11, 2023</span></div>\\n\\t<a target=\"_blank\" class=\"btn mb-5 font-sans text-sm no-underline\" href=\"https://github.com/huggingface/blog/blob/main/moe.md\">Update on GitHub</a></div><div class=\"not-prose\"><div class=\"SVELTE_HYDRATER contents\" data-props=\"{&quot;authors&quot;:[{&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png?w=200&amp;h=200&amp;f=face&quot;,&quot;fullname&quot;:&quot;Omar Sanseviero&quot;,&quot;name&quot;:&quot;osanseviero&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true}},{&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;fullname&quot;:&quot;Lewis Tunstall&quot;,&quot;name&quot;:&quot;lewtun&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:true}},{&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&amp;h=200&amp;f=face&quot;,&quot;fullname&quot;:&quot;Philipp Schmid&quot;,&quot;name&quot;:&quot;philschmid&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true}},{&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1638132956881-5fca176d1d7a08cb34d79d5d.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;fullname&quot;:&quot;Sourab Mangrulkar&quot;,&quot;name&quot;:&quot;smangrul&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true}},{&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&amp;h=200&amp;f=face&quot;,&quot;fullname&quot;:&quot;Younes Belkada&quot;,&quot;name&quot;:&quot;ybelkada&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true}},{&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;fullname&quot;:&quot;Pedro Cuenca&quot;,&quot;name&quot;:&quot;pcuenq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true}}],&quot;translators&quot;:[],&quot;proofreaders&quot;:[],&quot;lang&quot;:&quot;en&quot;}\" data-target=\"BlogAuthorsByline\"><div class=\"not-prose\"><div class=\"flex flex-wrap items-center gap-3.5\">\\n\\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/osanseviero\" class=\"flex items-center leading-tight\"><img class=\"m-0 mr-3 h-12 w-12 !rounded-full\" alt=\"Omar Sanseviero\\'s avatar\" src=\"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png?w=200&amp;h=200&amp;f=face\">\\n\\t\\t\\t\\t\\t<div class=\"font-thin text-gray-900 dark:text-gray-300\"><span class=\"block font-mono text-[0.92rem] !leading-tight underline lg:text-base\">osanseviero</span>\\n\\t\\t\\t\\t\\t\\t<span class=\"fullname underline\">Omar Sanseviero</span>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t</div></a>\\n\\t\\t\\t</span>\\n\\n\\t</span>\\n\\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lewtun\" class=\"flex items-center leading-tight\"><img class=\"m-0 mr-3 h-12 w-12 !rounded-full\" alt=\"Lewis Tunstall\\'s avatar\" src=\"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&amp;h=200&amp;f=face\">\\n\\t\\t\\t\\t\\t<div class=\"font-thin text-gray-900 dark:text-gray-300\"><span class=\"block font-mono text-[0.92rem] !leading-tight underline lg:text-base\">lewtun</span>\\n\\t\\t\\t\\t\\t\\t<span class=\"fullname underline\">Lewis Tunstall</span>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t</div></a>\\n\\t\\t\\t</span>\\n\\n\\t</span>\\n\\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/philschmid\" class=\"flex items-center leading-tight\"><img class=\"m-0 mr-3 h-12 w-12 !rounded-full\" alt=\"Philipp Schmid\\'s avatar\" src=\"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&amp;h=200&amp;f=face\">\\n\\t\\t\\t\\t\\t<div class=\"font-thin text-gray-900 dark:text-gray-300\"><span class=\"block font-mono text-[0.92rem] !leading-tight underline lg:text-base\">philschmid</span>\\n\\t\\t\\t\\t\\t\\t<span class=\"fullname underline\">Philipp Schmid</span>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t</div></a>\\n\\t\\t\\t</span>\\n\\n\\t</span>\\n\\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/smangrul\" class=\"flex items-center leading-tight\"><img class=\"m-0 mr-3 h-12 w-12 !rounded-full\" alt=\"Sourab Mangrulkar\\'s avatar\" src=\"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1638132956881-5fca176d1d7a08cb34d79d5d.jpeg?w=200&amp;h=200&amp;f=face\">\\n\\t\\t\\t\\t\\t<div class=\"font-thin text-gray-900 dark:text-gray-300\"><span class=\"block font-mono text-[0.92rem] !leading-tight underline lg:text-base\">smangrul</span>\\n\\t\\t\\t\\t\\t\\t<span class=\"fullname underline\">Sourab Mangrulkar</span>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t</div></a>\\n\\t\\t\\t</span>\\n\\n\\t</span>\\n\\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\" class=\"flex items-center leading-tight\"><img class=\"m-0 mr-3 h-12 w-12 !rounded-full\" alt=\"Younes Belkada\\'s avatar\" src=\"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&amp;h=200&amp;f=face\">\\n\\t\\t\\t\\t\\t<div class=\"font-thin text-gray-900 dark:text-gray-300\"><span class=\"block font-mono text-[0.92rem] !leading-tight underline lg:text-base\">ybelkada</span>\\n\\t\\t\\t\\t\\t\\t<span class=\"fullname underline\">Younes Belkada</span>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t</div></a>\\n\\t\\t\\t</span>\\n\\n\\t</span>\\n\\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pcuenq\" class=\"flex items-center leading-tight\"><img class=\"m-0 mr-3 h-12 w-12 !rounded-full\" alt=\"Pedro Cuenca\\'s avatar\" src=\"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&amp;h=200&amp;f=face\">\\n\\t\\t\\t\\t\\t<div class=\"font-thin text-gray-900 dark:text-gray-300\"><span class=\"block font-mono text-[0.92rem] !leading-tight underline lg:text-base\">pcuenq</span>\\n\\t\\t\\t\\t\\t\\t<span class=\"fullname underline\">Pedro Cuenca</span>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t</div></a>\\n\\t\\t\\t</span>\\n\\n\\t</span></div>\\n\\t</div></div></div>\\n<p>\\n\\n<div class=\"absolute -left-12 z-10 h-full not-prose hidden lg:block\"><div class=\"sticky top-4 flex\"><div class=\"pt-[0.175rem]\">\\n\\t\\t\\t\\t<span class=\"peer\" tabindex=\"0\"><button class=\"select-none hover:cursor-pointer\"><svg width=\"1em\" height=\"1em\" viewBox=\"0 0 10 10\" class=\"text-lg text-gray-400/60 dark:text-gray-500 hover:text-gray-800 dark:hover:text-gray-400\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" preserveAspectRatio=\"xMidYMid meet\" fill=\"currentColor\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z\"></path></svg></button></span>\\n\\t\\t\\t\\t<div class=\"invisible w-0 -translate-x-24 -translate-y-6 overflow-hidden rounded-xl border bg-white transition-transform hover:visible hover:w-52 hover:translate-x-0 peer-focus-within:visible peer-focus-within:w-52 peer-focus-within:translate-x-0\"><nav aria-label=\"Secondary\" class=\"max-h-[550px] overflow-y-auto p-3\"><ul><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#table-of-contents\" title=\"Table of Contents\"><!-- HTML_TAG_START -->Table of Contents<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#tldr\" title=\"TL;DR\"><!-- HTML_TAG_START -->TL;DR<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#what-is-a-mixture-of-experts-moe\" title=\"What is a Mixture of Experts (MoE)?\"><!-- HTML_TAG_START -->What is a Mixture of Experts (MoE)?<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#a-brief-history-of-moes\" title=\"A Brief History of MoEs\"><!-- HTML_TAG_START -->A Brief History of MoEs<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#what-is-sparsity\" title=\"What is Sparsity?\"><!-- HTML_TAG_START -->What is Sparsity?<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#load-balancing-tokens-for-moes\" title=\"Load balancing tokens for MoEs\"><!-- HTML_TAG_START -->Load balancing tokens for MoEs<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#moes-and-transformers\" title=\"MoEs and Transformers\"><!-- HTML_TAG_START -->MoEs and Transformers<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#switch-transformers\" title=\"Switch Transformers\"><!-- HTML_TAG_START -->Switch Transformers<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#stabilizing-training-with-router-z-loss\" title=\"Stabilizing training with router Z-loss\"><!-- HTML_TAG_START -->Stabilizing training with router Z-loss<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#what-does-an-expert-learn\" title=\"What does an expert learn?\"><!-- HTML_TAG_START -->What does an expert learn?<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#how-does-scaling-the-number-of-experts-impact-pretraining\" title=\"How does scaling the number of experts impact pretraining?\"><!-- HTML_TAG_START -->How does scaling the number of experts impact pretraining?<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#fine-tuning-moes\" title=\"Fine-tuning MoEs\"><!-- HTML_TAG_START -->Fine-tuning MoEs<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#when-to-use-sparse-moes-vs-dense-models\" title=\"When to use sparse MoEs vs dense models?\"><!-- HTML_TAG_START -->When to use sparse MoEs vs dense models?<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#making-moes-go-brrr\" title=\"Making MoEs go brrr\"><!-- HTML_TAG_START -->Making MoEs go brrr<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"><li><a class=\"mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500\" href=\"#parallelism\" title=\"Parallelism\"><!-- HTML_TAG_START -->Parallelism<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-2\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t</li><li><a class=\"mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500\" href=\"#capacity-factor-and-communication-costs\" title=\"Capacity Factor and communication costs\"><!-- HTML_TAG_START -->Capacity Factor and communication costs<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-2\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t</li><li><a class=\"mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500\" href=\"#serving-techniques\" title=\"Serving techniques\"><!-- HTML_TAG_START -->Serving techniques<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-2\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t</li><li><a class=\"mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500\" href=\"#more-on-efficient-training\" title=\"More on efficient training\"><!-- HTML_TAG_START -->More on efficient training<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-2\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t</li></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#open-source-moes\" title=\"Open Source MoEs\"><!-- HTML_TAG_START -->Open Source MoEs<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#exciting-directions-of-work\" title=\"Exciting directions of work\"><!-- HTML_TAG_START -->Exciting directions of work<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#some-resources\" title=\"Some resources\"><!-- HTML_TAG_START -->Some resources<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li><li class=\"mb-3 text-sm last:mb-0\"><a class=\"mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&>*]:break-words\" href=\"#citation\" title=\"Citation\"><!-- HTML_TAG_START -->Citation<!-- HTML_TAG_END --></a>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul class=\"pl-1\"></ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t</li></ul></nav></div></div></div></div>With the release of Mixtral 8x7B (<a href=\"https://mistral.ai/news/mixtral-of-experts/\">announcement</a>, <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\">model card</a>), a class of transformer has become the hottest topic in the open AI community: Mixture of Experts, or MoEs for short. In this blog post, we take a look at the building blocks of MoEs, how they’re trained, and the tradeoffs to consider when serving them for inference. </p>\\n<p>Let’s dive in!</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"table-of-contents\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#table-of-contents\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tTable of Contents\\n\\t</span>\\n</h2>\\n<ul>\\n<li><a href=\"#what-is-a-mixture-of-experts-moe\">What is a Mixture of Experts?</a></li>\\n<li><a href=\"#a-brief-history-of-moes\">A Brief History of MoEs</a></li>\\n<li><a href=\"#what-is-sparsity\">What is Sparsity?</a></li>\\n<li><a href=\"#load-balancing-tokens-for-moes\">Load Balancing tokens for MoEs</a></li>\\n<li><a href=\"#moes-and-transformers\">MoEs and Transformers</a></li>\\n<li><a href=\"#switch-transformers\">Switch Transformers</a></li>\\n<li><a href=\"#stabilizing-training-with-router-z-loss\">Stabilizing training with router Z-loss</a></li>\\n<li><a href=\"#what-does-an-expert-learn\">What does an expert learn?</a></li>\\n<li><a href=\"#how-does-scaling-the-number-of-experts-impact-pretraining\">How does scaling the number of experts impact pretraining?</a></li>\\n<li><a href=\"#fine-tuning-moes\">Fine-tuning MoEs</a></li>\\n<li><a href=\"#when-to-use-sparse-moes-vs-dense-models\">When to use sparse MoEs vs dense models?</a></li>\\n<li><a href=\"#making-moes-go-brrr\">Making MoEs go brrr</a><ul>\\n<li><a href=\"#parallelism\">Expert Parallelism</a></li>\\n<li><a href=\"#capacity-factor-and-communication-costs\">Capacity Factor and Communication costs</a></li>\\n<li><a href=\"#serving-techniques\">Serving Techniques</a></li>\\n<li><a href=\"#more-on-efficient-training\">Efficient Training</a></li>\\n</ul>\\n</li>\\n<li><a href=\"#open-source-moes\">Open Source MoEs</a></li>\\n<li><a href=\"#exciting-directions-of-work\">Exciting directions of work</a></li>\\n<li><a href=\"#some-resources\">Some resources</a></li>\\n</ul>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"tldr\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#tldr\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tTL;DR\\n\\t</span>\\n</h2>\\n<p>MoEs:</p>\\n<ul>\\n<li>Are <strong>pretrained much faster</strong> vs. dense models</li>\\n<li>Have <strong>faster inference</strong> compared to a model with the same number of parameters</li>\\n<li>Require <strong>high VRAM</strong> as all experts are loaded in memory</li>\\n<li>Face many <strong>challenges in fine-tuning</strong>, but <a href=\"https://arxiv.org/pdf/2305.14705.pdf\">recent work</a> with MoE <strong>instruction-tuning is promising</strong></li>\\n</ul>\\n<p>Let’s dive in!</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"what-is-a-mixture-of-experts-moe\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#what-is-a-mixture-of-experts-moe\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tWhat is a Mixture of Experts (MoE)?\\n\\t</span>\\n</h2>\\n<p>The scale of a model is one of the most important axes for better model quality. Given a fixed computing budget, training a larger model for fewer steps is better than training a smaller model for more steps. </p>\\n<p>Mixture of Experts enable models to be pretrained with far less compute, which means you can dramatically scale up the model or dataset size with the same compute budget as a dense model. In particular, a MoE model should achieve the same quality as its dense counterpart much faster during pretraining. </p>\\n<p>So, what exactly is a MoE? In the context of transformer models, a MoE consists of two main elements:</p>\\n<ul>\\n<li><strong>Sparse MoE layers</strong> are used instead of dense feed-forward network (FFN) layers. MoE layers have a certain number of “experts” (e.g. 8), where each expert is a neural network. In practice, the experts are FFNs, but they can also be more complex networks or even a MoE itself, leading to hierarchical MoEs!</li>\\n<li>A <strong>gate network or router</strong>, that determines which tokens are sent to which expert. For example, in the image below, the token “More” is sent to the second expert, and the token &quot;Parameters” is sent to the first network. As we’ll explore later, we can send a token to more than one expert. How to route a token to an expert is one of the big decisions when working with MoEs - the router is composed of learned parameters and is pretrained at the same time as the rest of the network.</li>\\n</ul>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/00_switch_transformer.png\" alt=\"Switch Layer\">\\n  <figcaption>MoE layer from the [Switch Transformers paper](https://arxiv.org/abs/2101.03961)</figcaption>\\n</figure>\\n\\n<p>So, to recap, in MoEs we replace every FFN layer of the transformer model with an MoE layer, which is composed of a gate network and a certain number of experts.</p>\\n<p>Although MoEs provide benefits like efficient pretraining and faster inference compared to dense models, they also come with challenges:</p>\\n<ul>\\n<li><strong>Training:</strong> MoEs enable significantly more compute-efficient pretraining, but they’ve historically struggled to generalize during fine-tuning, leading to overfitting.</li>\\n<li><strong>Inference:</strong> Although a MoE might have many parameters, only some of them are used during inference. This leads to much faster inference compared to a dense model with the same number of parameters. However, all parameters need to be loaded in RAM, so memory requirements are high. For example, given a MoE like Mixtral 8x7B, we’ll need to have enough VRAM to hold a dense 47B parameter model. Why 47B parameters and not 8 x 7B = 56B? That’s because in MoE models, only the FFN layers are treated as individual experts, and the rest of the model parameters are shared. At the same time, assuming just two experts are being used per token, the inference speed (FLOPs) is like using a 12B model (as opposed to a 14B model), because it computes 2x7B matrix multiplications, but with some layers shared (more on this soon).</li>\\n</ul>\\n<p>Now that we have a rough idea of what a MoE is, let’s take a look at the research developments that led to their invention.</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"a-brief-history-of-moes\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#a-brief-history-of-moes\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tA Brief History of MoEs\\n\\t</span>\\n</h2>\\n<p>The roots of MoEs come from the 1991 paper <a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive Mixture of Local Experts</a>. The idea, akin to ensemble methods, was to have a supervised procedure for a system composed of separate networks, each handling a different subset of the training cases. Each separate network, or expert, specializes in a different region of the input space. How is the expert chosen? A gating network determines the weights for each expert. During training, both the expert and the gating are trained.</p>\\n<p>Between 2010-2015, two different research areas contributed to later MoE advancement:</p>\\n<ul>\\n<li><strong>Experts as components</strong>: In the traditional MoE setup, the whole system comprises a gating network and multiple experts. MoEs as the whole model have been explored in SVMs, Gaussian Processes, and other methods. The work by <a href=\"https://arxiv.org/abs/1312.4314\">Eigen, Ranzato, and Ilya</a> explored MoEs as components of deeper networks. This allows having MoEs as layers in a multilayer network, making it possible for the model to be both large and efficient simultaneously.</li>\\n<li><strong>Conditional Computation</strong>: Traditional networks process all input data through every layer. In this period, Yoshua Bengio researched approaches to dynamically activate or deactivate components based on the input token.</li>\\n</ul>\\n<p>These works led to exploring a mixture of experts in the context of NLP. Concretely, <a href=\"https://arxiv.org/abs/1701.06538\">Shazeer et al.</a> (2017, with “et al.” including Geoffrey Hinton and Jeff Dean, <a href=\"https://www.informatika.bg/jeffdean\">Google’s Chuck Norris</a>) scaled this idea to a 137B LSTM (the de-facto NLP architecture back then, created by Schmidhuber) by introducing sparsity, allowing to keep very fast inference even at high scale. This work focused on translation but faced many challenges, such as high communication costs and training instabilities.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/01_moe_layer.png\" alt=\"MoE layer in LSTM\">\\n  <figcaption>MoE layer from the Outrageously Large Neural Network paper</figcaption>\\n</figure>\\n\\n<p>MoEs have allowed training multi-trillion parameter models, such as the open-sourced 1.6T parameters Switch Transformers, among others. MoEs have also been explored in Computer Vision, but this blog post will focus on the NLP domain.</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"what-is-sparsity\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#what-is-sparsity\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tWhat is Sparsity?\\n\\t</span>\\n</h2>\\n<p>Sparsity uses the idea of conditional computation. While in dense models all the parameters are used for all the inputs, sparsity allows us to only run some parts of the whole system.</p>\\n<p>Let’s dive deeper into Shazeer&#39;s exploration of MoEs for translation. The idea of conditional computation (parts of the network are active on a per-example basis) allows one to scale the size of the model without increasing the computation, and hence, this led to thousands of experts being used in each MoE layer.</p>\\n<p>This setup introduces some challenges. For example, although large batch sizes are usually better for performance, batch sizes in MOEs are effectively reduced as data flows through the active experts. For example, if our batched input consists of 10 tokens, <strong>five tokens might end in one expert, and the other five tokens might end in five different experts, leading to uneven batch sizes and underutilization</strong>. The <a href=\"#making-moes-go-brrr\">Making MoEs go brrr</a> section below will discuss other challenges and solutions.</p>\\n<p>How can we solve this? A learned gating network (G) decides which experts (E) to send a part of the input:</p>\\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>y</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>G</mi><mo stretchy=\"false\">(</mo><mi>x</mi><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub><msub><mi>E</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\ny = \\\\sum_{i=1}^{n} G(x)_i E_i(x)\\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">G</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span></p>\\n<p>In this setup, all experts are run for all inputs - it’s a weighted multiplication. But, what happens if G is 0? If that’s the case, there’s no need to compute the respective expert operations and hence we save compute. What’s a typical gating function? In the most traditional setup, we just use a simple network with a softmax function. The network will learn which expert to send the input.</p>\\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>G</mi><mi>σ</mi></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>Softmax</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo>⋅</mo><msub><mi>W</mi><mi>g</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\nG_\\\\sigma(x) = \\\\text{Softmax}(x \\\\cdot W_g)\\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">G</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">σ</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">Softmax</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">g</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\\n<p>Shazeer’s work also explored other gating mechanisms, such as Noisy Top-k Gating. This gating approach introduces some (tunable) noise and then keeps the top k values. That is:</p>\\n<ol>\\n<li>We add some noise</li>\\n</ol>\\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>H</mi><mo stretchy=\"false\">(</mo><mi>x</mi><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub><mo>=</mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo>⋅</mo><msub><mi>W</mi><mtext>g</mtext></msub><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub><mo>+</mo><mtext>StandardNormal()</mtext><mo>⋅</mo><mtext>Softplus</mtext><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo>⋅</mo><msub><mi>W</mi><mtext>noise</mtext></msub><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\nH(x)_i = (x \\\\cdot W_{\\\\text{g}})_i + \\\\text{StandardNormal()} \\\\cdot \\\\text{Softplus}((x \\\\cdot W_{\\\\text{noise}})_i)\\n\\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">g</span></span></span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">StandardNormal()</span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">Softplus</span></span><span class=\"mopen\">(</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31750199999999995em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">noise</span></span></span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\\n<ol start=\"2\">\\n<li>We only pick the top k</li>\\n</ol>\\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>KeepTopK</mtext><mo stretchy=\"false\">(</mo><mi>v</mi><mo separator=\"true\">,</mo><mi>k</mi><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.3599999999999999em\" columnalign=\"left left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi>v</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if\\xa0</mtext><msub><mi>v</mi><mi>i</mi></msub><mtext>\\xa0is\\xa0in\\xa0the\\xa0top\\xa0</mtext><mi>k</mi><mtext>\\xa0elements\\xa0of\\xa0</mtext><mi>v</mi><mo separator=\"true\">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mo>−</mo><mi mathvariant=\"normal\">∞</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mtext>otherwise.</mtext></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">\\n\\\\text{KeepTopK}(v, k)_i = \\\\begin{cases}\\nv_i &amp; \\\\text{if } v_i \\\\text{ is in the top } k \\\\text{ elements of } v, \\\\\\\\\\n-\\\\infty &amp; \\\\text{otherwise.}\\n\\\\end{cases}\\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">KeepTopK</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.0000299999999998em;vertical-align:-1.25003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.69em;\"><span style=\"top:-3.69em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.25em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord\">∞</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.19em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:1em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.69em;\"><span style=\"top:-3.69em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">if\\xa0</span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord text\"><span class=\"mord\">\\xa0is\\xa0in\\xa0the\\xa0top\\xa0</span></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord text\"><span class=\"mord\">\\xa0elements\\xa0of\\xa0</span></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mpunct\">,</span></span></span><span style=\"top:-2.25em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">otherwise.</span></span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.19em;\"><span></span></span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\\n<ol start=\"3\">\\n<li>We apply the softmax.</li>\\n</ol>\\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>G</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>Softmax</mtext><mo stretchy=\"false\">(</mo><mtext>KeepTopK</mtext><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mi>k</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\nG(x) = \\\\text{Softmax}(\\\\text{KeepTopK}(H(x), k))\\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">G</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">Softmax</span></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">KeepTopK</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span></span></p>\\n<p>This sparsity introduces some interesting properties. By using a low enough k (e.g. one or two), we can train and run inference much faster than if many experts were activated. Why not just select the top expert? The initial conjecture was that routing to more than one expert was needed to have the gate learn how to route to different experts, so at least two experts had to be picked. The <a href=\"#switch-transformers\">Switch Transformers</a> section revisits this decision.</p>\\n<p>Why do we add noise? That’s for load balancing!</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"load-balancing-tokens-for-moes\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#load-balancing-tokens-for-moes\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tLoad balancing tokens for MoEs\\n\\t</span>\\n</h2>\\n<p>As discussed before, if all our tokens are sent to just a few popular experts, that will make training inefficient. In a normal MoE training, the gating network converges to mostly activate the same few experts. This self-reinforces as favored experts are trained quicker and hence selected more. To mitigate this, an <strong>auxiliary loss</strong> is added to encourage giving all experts equal importance. This loss ensures that all experts receive a roughly equal number of training examples. The following sections will also explore the concept of expert capacity, which introduces a threshold of how many tokens can be processed by an expert. In <code>transformers</code>, the auxiliary loss is exposed via the <code>aux_loss</code> parameter.</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"moes-and-transformers\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#moes-and-transformers\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tMoEs and Transformers\\n\\t</span>\\n</h2>\\n<p>Transformers are a very clear case that scaling up the number of parameters improves the performance, so it’s not surprising that Google explored this with <a href=\"https://arxiv.org/abs/2006.16668\">GShard</a>, which explores scaling up transformers beyond 600 billion parameters.</p>\\n<p>GShard replaces every other FFN layer with an MoE layer using top-2 gating in both the encoder and the decoder. The next image shows how this looks like for the encoder part. This setup is quite beneficial for large-scale computing: when we scale to multiple devices, the MoE layer is shared across devices while all the other layers are replicated. This is further discussed in the <a href=\"#making-moes-go-brrr\">“Making MoEs go brrr”</a> section.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/02_moe_block.png\" alt=\"MoE Transformer Encoder\">\\n  <figcaption>MoE Transformer Encoder from the GShard Paper</figcaption>\\n</figure>\\n\\n<p>To maintain a balanced load and efficiency at scale, the GShard authors introduced a couple of changes in addition to an auxiliary loss similar to the one discussed in the previous section:</p>\\n<ul>\\n<li><strong>Random routing</strong>: in a top-2 setup, we always pick the top expert, but the second expert is picked with probability proportional to its weight.</li>\\n<li><strong>Expert capacity</strong>: we can set a threshold of how many tokens can be processed by one expert. If both experts are at capacity, the token is considered overflowed, and it’s sent to the next layer via residual connections (or dropped entirely in other projects). This concept will become one of the most important concepts for MoEs. Why is expert capacity needed? Since all tensor shapes are statically determined at compilation time, but we cannot know how many tokens will go to each expert ahead of time, we need to fix the capacity factor.</li>\\n</ul>\\n<p>The GShard paper has contributions by expressing parallel computation patterns that work well for MoEs, but discussing that is outside the scope of this blog post.</p>\\n<p><strong>Note:</strong> when we run inference, only some experts will be triggered. At the same time, there are shared computations, such as self-attention, which is applied for all tokens. That’s why when we talk of a 47B model of 8 experts, we can run with the compute of a 12B dense model. If we use top-2, 14B parameters would be used. But given that the attention operations are shared (among others), the actual number of used parameters is 12B.</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"switch-transformers\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#switch-transformers\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tSwitch Transformers\\n\\t</span>\\n</h2>\\n<p>Although MoEs showed a lot of promise, they struggle with training and fine-tuning instabilities. <a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers</a> is a very exciting work that deep dives into these topics. The authors even released a <a href=\"https://huggingface.co/google/switch-c-2048\">1.6 trillion parameters MoE on Hugging Face</a> with 2048 experts, which you can run with transformers. Switch Transformers achieved a 4x pre-train speed-up over T5-XXL.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/03_switch_layer.png\" alt=\"Switch Transformer Layer\">\\n  <figcaption>Switch Transformer Layer of the Switch Transformer paper</figcaption>\\n</figure>\\n\\n<p>Just as in GShard, the authors replaced the FFN layers with a MoE layer. The Switch Transformers paper proposes a Switch Transformer layer that receives two inputs (two different tokens) and has four experts.</p>\\n<p>Contrary to the initial idea of using at least two experts, Switch Transformers uses a simplified single-expert strategy. The effects of this approach are:</p>\\n<ul>\\n<li>The router computation is reduced</li>\\n<li>The batch size of each expert can be at least halved</li>\\n<li>Communication costs are reduced</li>\\n<li>Quality is preserved</li>\\n</ul>\\n<p>Switch Transformers also explores the concept of expert capacity. </p>\\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Expert\\xa0Capacity</mtext><mo>=</mo><mrow><mo fence=\"true\">(</mo><mfrac><mtext>tokens\\xa0per\\xa0batch</mtext><mtext>number\\xa0of\\xa0experts</mtext></mfrac><mo fence=\"true\">)</mo></mrow><mo>×</mo><mtext>capacity\\xa0factor</mtext></mrow><annotation encoding=\"application/x-tex\">\\n\\\\text{Expert Capacity} = \\\\left(\\\\frac{\\\\text{tokens per batch}}{\\\\text{number of experts}}\\\\right) \\\\times \\\\text{capacity factor}\\n\\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">Expert\\xa0Capacity</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714399999999998em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">number\\xa0of\\xa0experts</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">tokens\\xa0per\\xa0batch</span></span></span></span></span><span class=\"vlist-s\">\\u200b</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8804400000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">capacity\\xa0factor</span></span></span></span></span></span></p>\\n<p>The capacity suggested above evenly divides the number of tokens in the batch across the number of experts. If we use a capacity factor greater than 1, we provide a buffer for when tokens are not perfectly balanced. Increasing the capacity will lead to more expensive inter-device communication, so it’s a trade-off to keep in mind. In particular, Switch Transformers perform well at low capacity factors (1-1.25)</p>\\n<p>Switch Transformer authors also revisit and simplify the load balancing loss mentioned in the sections. For each Switch layer, the auxiliary loss is added to the total model loss during training. This loss encourages uniform routing and can be weighted using a hyperparameter.</p>\\n<p>The authors also experiment with selective precision, such as training the experts with <code>bfloat16</code> while using full precision for the rest of the computations. Lower precision reduces communication costs between processors, computation costs, and memory for storing tensors. The initial experiments, in which both the experts and the gate networks were trained in <code>bfloat16</code>, yielded more unstable training. This was, in particular, due to the router computation: as the router has an exponentiation function, having higher precision is important. To mitigate the instabilities, full precision was used for the routing as well.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/04_switch_table.png\" alt=\"Table shows that selective precision does not degrade quality.\">\\n  <figcaption>Using selective precision does not degrade quality and enables faster models</figcaption>\\n</figure>\\n\\n<p>This <a href=\"https://colab.research.google.com/drive/1aGGVHZmtKmcNBbAwa9hbu58DDpIuB5O4?usp=sharing\">notebook</a> showcases fine-tuning Switch Transformers for summarization, but we suggest first reviewing the <a href=\"#fine-tuning-moes\">fine-tuning section</a>.</p>\\n<p>Switch Transformers uses an encoder-decoder setup in which they did a MoE counterpart of T5. The <a href=\"https://arxiv.org/abs/2112.06905\">GLaM</a> paper explores pushing up the scale of these models by training a model matching GPT-3 quality using 1/3 of the energy (yes, thanks to the lower amount of computing needed to train a MoE, they can reduce the carbon footprint by up to an order of magnitude). The authors focused on decoder-only models and few-shot and one-shot evaluation rather than fine-tuning. They used Top-2 routing and much larger capacity factors. In addition, they explored the capacity factor as a metric one can change during training and evaluation depending on how much computing one wants to use. </p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"stabilizing-training-with-router-z-loss\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#stabilizing-training-with-router-z-loss\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tStabilizing training with router Z-loss\\n\\t</span>\\n</h2>\\n<p>The balancing loss previously discussed can lead to instability issues. We can use many methods to stabilize sparse models at the expense of quality. For example, introducing dropout improves stability but leads to loss of model quality. On the other hand, adding more multiplicative components improves quality but decreases stability.</p>\\n<p>Router z-loss, introduced in <a href=\"https://arxiv.org/abs/2202.08906\">ST-MoE</a>, significantly improves training stability without quality degradation by penalizing large logits entering the gating network. Since this loss encourages absolute magnitude of values to be smaller, roundoff errors are reduced, which can be quite impactful for exponential functions such as the gating. We recommend reviewing the paper for details.</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"what-does-an-expert-learn\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#what-does-an-expert-learn\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tWhat does an expert learn?\\n\\t</span>\\n</h2>\\n<p>The ST-MoE authors observed that encoder experts specialize in a group of tokens or shallow concepts. For example, we might end with a punctuation expert, a proper noun expert, etc. On the other hand, the decoder experts have less specialization. The authors also trained in a multilingual setup. Although one could imagine each expert specializing in a language, the opposite happens: due to token routing and load balancing, there is no single expert specialized in any given language.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/05_experts_learning.png\" alt=\"Experts specialize in some token groups\">\\n  <figcaption>Table from the ST-MoE paper showing which token groups were sent to which expert.</figcaption>\\n</figure>\\n\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"how-does-scaling-the-number-of-experts-impact-pretraining\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#how-does-scaling-the-number-of-experts-impact-pretraining\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tHow does scaling the number of experts impact pretraining?\\n\\t</span>\\n</h2>\\n<p>More experts lead to improved sample efficiency and faster speedup, but these are diminishing gains (especially after 256 or 512), and more VRAM will be needed for inference. The properties studied in Switch Transformers at large scale were consistent at small scale, even with 2, 4, or 8 experts per layer.</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"fine-tuning-moes\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#fine-tuning-moes\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tFine-tuning MoEs\\n\\t</span>\\n</h2>\\n<blockquote>\\n<p>Mixtral is supported with version 4.36.0 of transformers. You can install it with <code>pip install &quot;transformers==4.36.0 --upgrade</code></p>\\n</blockquote>\\n<p>The overfitting dynamics are very different between dense and sparse models. Sparse models are more prone to overfitting, so we can explore higher regularization (e.g. dropout) within the experts themselves (e.g. we can have one dropout rate for the dense layers and another, higher, dropout for the sparse layers). </p>\\n<p>One question is whether to use the auxiliary loss for fine-tuning. The ST-MoE authors experimented with turning off the auxiliary loss, and the quality was not significantly impacted, even when up to 11% of the tokens were dropped. Token dropping might be a form of regularization that helps prevent overfitting. </p>\\n<p>Switch Transformers observed that at a fixed pretrain perplexity, the sparse model does worse than the dense counterpart in downstream tasks, especially on reasoning-heavy tasks such as SuperGLUE. On the other hand, for knowledge-heavy tasks such as TriviaQA, the sparse model performs disproportionately well. The authors also observed that a fewer number of experts helped at fine-tuning. Another observation that confirmed the generalization issue is that the model did worse in smaller tasks but did well in larger tasks.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/06_superglue_curves.png\" alt=\"Fine-tuning learning curves\">\\n  <figcaption>In the small task (left), we can see clear overfitting as the sparse model does much worse in the validation set. In the larger task (right), the MoE performs well. This image is from the ST-MoE paper.</figcaption>\\n</figure>\\n\\n\\n<p>One could experiment with freezing all non-expert weights. That is, we&#39;ll only update the MoE layers. This leads to a huge performance drop. We could try the opposite: freezing only the parameters in MoE layers, which worked almost as well as updating all parameters. This can help speed up and reduce memory for fine-tuning. This can be somewhat counter-intuitive as 80% of the parameters are in the MoE layers (in the ST-MoE project). Their hypothesis for that architecture is that, as expert layers only occur every 1/4 layers, and each token sees at most two experts per layer, updating the MoE parameters affects much fewer layers than updating other parameters.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/07_superglue_bars.png\" alt=\"Only updating the non MoE layers works well in fine-tuning\">\\n  <figcaption>By only freezing the MoE layers, we can speed up the training while preserving the quality. This image is from the ST-MoE paper.</figcaption>\\n</figure>\\n\\n<p>One last part to consider when fine-tuning sparse MoEs is that they have different fine-tuning hyperparameter setups - e.g., sparse models tend to benefit more from smaller batch sizes and higher learning rates.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/08_superglue_dense_vs_sparse.png\" alt=\"Table comparing fine-tuning batch size and learning rate between dense and sparse models.\">\\n  <figcaption>Sparse models fine-tuned quality improves with higher learning rates and smaller batch sizes. This image is from the ST-MoE paper.</figcaption>\\n</figure>\\n\\n<p>At this point, you might be a bit sad that people have struggled to fine-tune MoEs. Excitingly, a recent paper, <a href=\"https://arxiv.org/pdf/2305.14705.pdf\">MoEs Meets Instruction Tuning</a> (July 2023), performs experiments doing:</p>\\n<ul>\\n<li>Single task fine-tuning</li>\\n<li>Multi-task instruction-tuning</li>\\n<li>Multi-task instruction-tuning followed by single-task fine-tuning</li>\\n</ul>\\n<p>When the authors fine-tuned the MoE and the T5 equivalent, the T5 equivalent was better. When the authors fine-tuned the Flan T5 (T5 instruct equivalent) MoE, the MoE performed significantly better. Not only this, the improvement of the Flan-MoE over the MoE was larger than Flan T5 over T5, indicating that MoEs might benefit much more from instruction tuning than dense models. MoEs benefit more from a higher number of tasks.  Unlike the previous discussion suggesting to turn off the auxiliary loss function, the loss actually prevents overfitting.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/09_fine_tune_evals.png\" alt=\"MoEs benefit even more from instruct tuning than dense models\">\\n  <figcaption>Sparse models benefit more from instruct-tuning compared to dense models. This image is from the MoEs Meets Instruction Tuning paper</figcaption>\\n</figure>\\n\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"when-to-use-sparse-moes-vs-dense-models\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#when-to-use-sparse-moes-vs-dense-models\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tWhen to use sparse MoEs vs dense models?\\n\\t</span>\\n</h2>\\n<p>Experts are useful for high throughput scenarios with many machines. Given a fixed compute budget for pretraining, a sparse model will be more optimal. For low throughput scenarios with little VRAM, a dense model will be better. </p>\\n<p><strong>Note:</strong> one cannot directly compare the number of parameters between sparse and dense models, as both represent significantly different things.</p>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"making-moes-go-brrr\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#making-moes-go-brrr\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tMaking MoEs go brrr\\n\\t</span>\\n</h2>\\n<p>The initial MoE work presented MoE layers as a branching setup, leading to slow computation as GPUs are not designed for it and leading to network bandwidth becoming a bottleneck as the devices need to send info to others. This section will discuss some existing work to make pretraining and inference with these models more practical. MoEs go brrrrr.</p>\\n<h3 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"parallelism\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#parallelism\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tParallelism\\n\\t</span>\\n</h3>\\n<p>Let’s do a brief review of parallelism:</p>\\n<ul>\\n<li><strong>Data parallelism:</strong> the same weights are replicated across all cores, and the data is partitioned across cores.</li>\\n<li><strong>Model parallelism:</strong> the model is partitioned across cores, and the data is replicated across cores.</li>\\n<li><strong>Model and data parallelism:</strong> we can partition the model and the data across cores. Note that different cores process different batches of data.</li>\\n<li><strong>Expert parallelism</strong>: experts are placed on different workers. If combined with data parallelism, each core has a different expert and the data is partitioned across all cores</li>\\n</ul>\\n<p>With expert parallelism, experts are placed on different workers, and each worker takes a different batch of training samples. For non-MoE layers, expert parallelism behaves the same as data parallelism. For MoE layers, tokens in the sequence are sent to workers where the desired experts reside.</p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/10_parallelism.png\" alt=\"Image illustrating model, expert, and data prallelism\">\\n  <figcaption>Illustration from the Switch Transformers paper showing how data and models are split over cores with different parallelism techniques.</figcaption>\\n</figure>\\n\\n<h3 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"capacity-factor-and-communication-costs\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#capacity-factor-and-communication-costs\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tCapacity Factor and communication costs\\n\\t</span>\\n</h3>\\n<p>Increasing the capacity factor (CF) increases the quality but increases communication costs and memory of activations. If all-to-all communications are slow, using a smaller capacity factor is better. A good starting point is using top-2 routing with 1.25 capacity factor and having one expert per core. During evaluation, the capacity factor can be changed to reduce compute.</p>\\n<h3 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"serving-techniques\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#serving-techniques\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tServing techniques\\n\\t</span>\\n</h3>\\n<blockquote>\\n<p>You can deploy <a href=\"https://ui.endpoints.huggingface.co/new?repository=mistralai%2FMixtral-8x7B-Instruct-v0.1&amp;vendor=aws&amp;region=us-east-1&amp;accelerator=gpu&amp;instance_size=2xlarge&amp;task=text-generation&amp;no_suggested_compute=true&amp;tgi=true&amp;tgi_max_batch_total_tokens=1024000&amp;tgi_max_total_tokens=32000\">mistralai/Mixtral-8x7B-Instruct-v0.1</a> to Inference Endpoints. </p>\\n</blockquote>\\n<p>A big downside of MoEs is the large number of parameters. For local use cases, one might want to use a smaller model. Let&#39;s quickly discuss a few techniques that can help with serving:</p>\\n<ul>\\n<li>The Switch Transformers authors did early distillation experiments. By distilling a MoE back to its dense counterpart, they could keep 30-40% of the sparsity gains. Distillation, hence, provides the benefits of faster pretaining and using a smaller model in production.</li>\\n<li>Recent approaches modify the routing to route full sentences or tasks to an expert, permitting extracting sub-networks for serving.</li>\\n<li>Aggregation of Experts (MoE): this technique merges the weights of the experts, hence reducing the number of parameters at inference time.</li>\\n</ul>\\n<h3 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"more-on-efficient-training\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#more-on-efficient-training\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tMore on efficient training\\n\\t</span>\\n</h3>\\n<p>FasterMoE (March 2022) analyzes the performance of MoEs in highly efficient distributed systems and analyzes the theoretical limit of different parallelism strategies, as well as techniques to skew expert popularity, fine-grained schedules of communication that reduce latency, and an adjusted topology-aware gate that picks experts based on the lowest latency, leading to a 17x speedup.</p>\\n<p>Megablocks (Nov 2022) explores efficient sparse pretraining by providing new GPU kernels that can handle the dynamism present in MoEs. Their proposal never drops tokens and maps efficiently to modern hardware, leading to significant speedups. What’s the trick? Traditional MoEs use batched matrix multiplication, which assumes all experts have the same shape and the same number of tokens. In contrast, Megablocks expresses MoE layers as block-sparse operations that can accommodate imbalanced assignment. </p>\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/moe/11_expert_matmuls.png\" alt=\"Matrix multiplication optimized for block-sparse operations.\">\\n  <figcaption>Block-sparse matrix multiplication for differently sized experts and number of tokens (from [MegaBlocks](https://arxiv.org/abs/2211.15841)).</figcaption>\\n</figure>\\n\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"open-source-moes\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#open-source-moes\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tOpen Source MoEs\\n\\t</span>\\n</h2>\\n<p>There are nowadays several open source projects to train MoEs:</p>\\n<ul>\\n<li>Megablocks: <a href=\"https://github.com/stanford-futuredata/megablocks\">https://github.com/stanford-futuredata/megablocks</a></li>\\n<li>Fairseq: <a href=\"https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm\">https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm</a></li>\\n<li>OpenMoE: <a href=\"https://github.com/XueFuzhao/OpenMoE\">https://github.com/XueFuzhao/OpenMoE</a></li>\\n</ul>\\n<p>In the realm of released open access MoEs, you can check:</p>\\n<ul>\\n<li><a href=\"https://huggingface.co/collections/google/switch-transformers-release-6548c35c6507968374b56d1f\">Switch Transformers (Google)</a>: Collection of T5-based MoEs going from 8 to 2048 experts. The largest model has 1.6 trillion parameters.</li>\\n<li><a href=\"https://huggingface.co/facebook/nllb-moe-54b\">NLLB MoE (Meta)</a>: A MoE variant of the NLLB translation model.</li>\\n<li><a href=\"https://huggingface.co/fuzhao\">OpenMoE</a>: A community effort that has released Llama-based MoEs.</li>\\n<li><a href=\"https://huggingface.co/mistralai\">Mixtral 8x7B (Mistral)</a>: A high-quality MoE that outperforms Llama 2 70B and has much faster inference. A instruct-tuned model is also released. Read more about it in <a href=\"https://mistral.ai/news/mixtral-of-experts/\">the announcement blog post</a>.</li>\\n</ul>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"exciting-directions-of-work\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#exciting-directions-of-work\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tExciting directions of work\\n\\t</span>\\n</h2>\\n<p>Further experiments on <strong>distilling</strong> a sparse MoE back to a dense model with less parameters but similar number of parameters.</p>\\n<p>Another area will be quantization of MoEs. <a href=\"https://arxiv.org/abs/2310.16795\">QMoE</a> (Oct. 2023) is a good step in this direction by quantizing the MoEs to less than 1 bit per parameter, hence compressing the 1.6T Switch Transformer which uses 3.2TB accelerator to just 160GB. </p>\\n<p>So, TL;DR, some interesting areas to explore:</p>\\n<ul>\\n<li>Distilling Mixtral into a dense model</li>\\n<li>Explore model merging techniques of the experts and their impact in inference time</li>\\n<li>Perform extreme quantization techniques of Mixtral</li>\\n</ul>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"some-resources\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#some-resources\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tSome resources\\n\\t</span>\\n</h2>\\n<ul>\\n<li><a href=\"https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf\">Adaptive Mixture of Local Experts (1991)</a></li>\\n<li><a href=\"https://arxiv.org/abs/1312.4314\">Learning Factored Representations in a Deep Mixture of Experts (2013)</a></li>\\n<li><a href=\"https://arxiv.org/abs/1701.06538\">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017)</a></li>\\n<li><a href=\"https://arxiv.org/abs/2006.16668\">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Jun 2020)</a></li>\\n<li><a href=\"https://arxiv.org/abs/2112.06905\">GLaM: Efficient Scaling of Language Models with Mixture-of-Experts (Dec 2021)</a></li>\\n<li><a href=\"https://arxiv.org/abs/2101.03961\">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2022)</a></li>\\n<li><a href=\"https://arxiv.org/abs/2202.08906\">ST-MoE: Designing Stable and Transferable Sparse Expert Models (Feb 2022)</a></li>\\n<li><a href=\"https://dl.acm.org/doi/10.1145/3503221.3508418\">FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models(April 2022)</a></li>\\n<li><a href=\"https://arxiv.org/abs/2211.15841\">MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (Nov 2022)</a></li>\\n<li><a href=\"https://arxiv.org/abs/2305.14705\">Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models (May 2023)</a></li>\\n<li><a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\">Mixtral-8x7B-v0.1</a>, <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\">Mixtral-8x7B-Instruct-v0.1</a>.</li>\\n</ul>\\n<h2 class=\"relative group flex items-center\">\\n\\t<a \\n\\t\\tid=\"citation\" \\n\\t\\tclass=\"block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full\" \\n\\t\\thref=\"#citation\"\\n\\t>\\n\\t\\t<span class=\"header-link\"><svg class=\"text-gray-500 hover:text-black dark:hover:text-gray-200 w-4\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 256 256\"><path d=\"M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z\" fill=\"currentColor\"></path></svg></span>\\n\\t</a>\\n\\t<span>\\n\\t\\tCitation\\n\\t</span>\\n</h2>\\n<pre><code class=\"language-bibtex\">@misc {sanseviero2023moe,\\n    author       = { Omar Sanseviero and\\n                     Lewis Tunstall and\\n                     Philipp Schmid and\\n                     Sourab Mangrulkar and\\n                     Younes Belkada and\\n                     Pedro Cuenca\\n                   },\\n    title        = { Mixture of Experts Explained },\\n    year         = 2023,\\n    url          = { https://huggingface.co/blog/moe },\\n    publisher    = { Hugging Face Blog }\\n}\\n</code></pre>\\n<pre><code>Sanseviero, et al., &quot;Mixture of Experts Explained&quot;, Hugging Face Blog, 2023.\\n</code></pre>\\n<!-- HTML_TAG_END --></div>\\n\\t<div class=\"mx-auto max-w-5xl border-t border-gray-200 py-16\"><div class=\"container grid gap-4 py-8\"><div class=\"grid gap-6 md:grid-cols-2\"><p class=\"col-span-1 mb-6 text-center text-lg font-semibold md:col-span-2\">More articles from our Blog</p>\\n\\t\\t\\t\\t\\t<div class=\"SVELTE_HYDRATER contents\" data-props=\"{&quot;blog&quot;:{&quot;local&quot;:&quot;segmoe&quot;,&quot;title&quot;:&quot;SegMoE: Segmind Mixture of Diffusion Experts&quot;,&quot;author&quot;:&quot;Warlord-K&quot;,&quot;guest&quot;:true,&quot;thumbnail&quot;:&quot;/blog/assets/segmoe/thumbnail.png&quot;,&quot;date&quot;:&quot;February 3, 2024&quot;,&quot;tags&quot;:[&quot;text-to-image&quot;,&quot;stable-diffusion&quot;,&quot;moe&quot;,&quot;segmoe&quot;]},&quot;blogUrl&quot;:&quot;/blog&quot;,&quot;lang&quot;:&quot;en&quot;}\" data-target=\"BlogThumbnail\"><a class=\"flex lg:col-span-1 hover:shadow-alternate group relative flex-col overflow-hidden rounded-xl border border-gray-100 shadow-sm transition-shadow\" href=\"/blog/segmoe\"><div class=\"aspect-[1.91/1] w-full rounded-b shadow-alternate relative flex-none overflow-hidden rounded-lg bg-white\"><div class=\"absolute inset-0 group-hover:opacity-40 dark:backdrop-brightness-105\"></div>\\n\\n\\t\\t<img src=\"/blog/assets/segmoe/thumbnail.png\" class=\"h-full w-full object-cover group-hover:brightness-110\" alt=\"\"></div>\\n\\n\\t<div class=\"flex flex-col p-4\"><h2 class=\"font-serif font-semibold group-hover:underline text-xl\">SegMoE: Segmind Mixture of Diffusion Experts\\n\\t\\t\\t</h2>\\n\\n\\t\\t<p class=\"mt-3 flex flex-wrap items-center gap-y-1.5 font-mono text-xs text-gray-500\">By\\xa0<object title=\"\">\\n\\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Warlord-K\" class=\"hover:underline\">Warlord-K</a></span>\\n\\n\\t</span></object>\\n\\t\\t\\t\\t<span class=\"mx-2 h-1 w-1 flex-none bg-gray-200\"></span>\\n\\t\\t\\t<span>February 3, 2024</span>\\n\\t\\t\\t<span class=\"mx-2 h-1 w-1 flex-none bg-gray-200\"></span>\\n\\t\\t\\t\\t<span class=\"rounded bg-gray-100 px-1 text-gray-800 dark:bg-gray-800\">guest</span></p></div></a>\\n\\t</div><div class=\"SVELTE_HYDRATER contents\" data-props=\"{&quot;blog&quot;:{&quot;local&quot;:&quot;mixtral&quot;,&quot;title&quot;:&quot;Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face&quot;,&quot;author&quot;:&quot;lewtun&quot;,&quot;thumbnail&quot;:&quot;/blog/assets/mixtral/thumbnail.jpg&quot;,&quot;date&quot;:&quot;December 11, 2023&quot;,&quot;tags&quot;:[&quot;mixtral&quot;,&quot;moe&quot;,&quot;nlp&quot;,&quot;llm&quot;,&quot;transformers&quot;]},&quot;blogUrl&quot;:&quot;/blog&quot;,&quot;lang&quot;:&quot;en&quot;}\" data-target=\"BlogThumbnail\"><a class=\"flex lg:col-span-1 hover:shadow-alternate group relative flex-col overflow-hidden rounded-xl border border-gray-100 shadow-sm transition-shadow\" href=\"/blog/mixtral\"><div class=\"aspect-[1.91/1] w-full rounded-b shadow-alternate relative flex-none overflow-hidden rounded-lg bg-white\"><div class=\"absolute inset-0 group-hover:opacity-40 dark:backdrop-brightness-105\"></div>\\n\\n\\t\\t<img src=\"/blog/assets/mixtral/thumbnail.jpg\" class=\"h-full w-full object-cover group-hover:brightness-110\" alt=\"\"></div>\\n\\n\\t<div class=\"flex flex-col p-4\"><h2 class=\"font-serif font-semibold group-hover:underline text-xl\">Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face\\n\\t\\t\\t</h2>\\n\\n\\t\\t<p class=\"mt-3 flex flex-wrap items-center gap-y-1.5 font-mono text-xs text-gray-500\">By\\xa0<object title=\"\">\\n\\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lewtun\" class=\"hover:underline\">lewtun</a></span>\\n\\n\\t</span></object>\\n\\t\\t\\t\\t<span class=\"mx-2 h-1 w-1 flex-none bg-gray-200\"></span>\\n\\t\\t\\t<span>December 11, 2023</span>\\n\\t\\t\\t</p></div></a>\\n\\t</div></div></div></div></main>\\n\\t<footer class=\"b-12 mb-2 flex border-t border-gray-100 md:h-14\"><nav class=\"container flex flex-col justify-between space-y-2 py-6 text-gray-500 md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm\"><div class=\"font-semibold text-black md:hidden\">Company</div>\\n\\t\\t<div class=\"order-last pt-6 text-gray-400 md:order-none md:pt-0\" href=\"Terms\">© Hugging Face</div>\\n\\t\\t<a class=\"hover:underline\" href=\"/terms-of-service\">TOS</a>\\n\\t\\t<a class=\"hover:underline\" href=\"/privacy\">Privacy</a>\\n\\t\\t<a class=\"hover:underline\" href=\"/huggingface\">About</a>\\n\\t\\t<a class=\"hover:underline\" href=\"https://apply.workable.com/huggingface/\">Jobs</a>\\n\\t\\t<a href=\"/\" class=\"group order-first flex-none pb-6 md:order-none md:pb-0\"><svg class=\"h-7 w-7 transition-transform group-hover:-translate-y-px\" viewBox=\"0 0 95 88\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z\" fill=\"#FFD21E\"></path><path d=\"M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z\" fill=\"#FF9D0B\"></path><path d=\"M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z\" fill=\"#3A3B45\"></path><path d=\"M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z\" fill=\"#3A3B45\"></path><path d=\"M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z\" fill=\"#3A3B45\"></path><mask id=\"mask0\" mask-type=\"alpha\" maskUnits=\"userSpaceOnUse\" x=\"33\" y=\"41\" width=\"27\" height=\"16\"><path d=\"M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z\" fill=\"white\"></path></mask><g mask=\"url(#mask0)\"><path d=\"M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z\" fill=\"#F94040\"></path></g><path d=\"M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z\" fill=\"#FF9D0B\"></path><path d=\"M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z\" fill=\"#FF9D0B\"></path><path class=\"origin-bottom-right transition-transform group-hover:-rotate-6\" d=\"M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z\" fill=\"#FF9D0B\"></path><path class=\"origin-bottom-right transition-transform group-hover:-rotate-6\" d=\"M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z\" fill=\"#FFD21E\"></path><path class=\"origin-bottom-left transition-transform group-hover:rotate-6\" d=\"M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z\" fill=\"#FF9D0B\"></path><path class=\"origin-bottom-left transition-transform group-hover:rotate-6\" d=\"M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z\" fill=\"#FFD21E\"></path></svg></a>\\n\\t\\t<div class=\"pt-6 font-semibold text-black md:hidden md:pt-0\">Website</div>\\n\\n\\t\\t<a class=\"hover:underline\" href=\"/models\">Models</a>\\n\\t\\t<a class=\"hover:underline\" href=\"/datasets\">Datasets</a>\\n\\t\\t<a class=\"hover:underline\" href=\"/spaces\">Spaces</a>\\n\\t\\t<a class=\"hover:underline\" href=\"/pricing\">Pricing</a>\\n\\t\\t<a class=\"hover:underline\" href=\"/docs\">Docs</a></nav></footer></div>\\n\\n\\t\\t<script>\\n\\t\\t\\timport(\"/front/build/kube-78db83e/index.js\");\\n\\t\\t\\twindow.moonSha = \"kube-78db83e/\";\\n\\t\\t\\twindow.hubConfig = JSON.parse(`{\"features\":{\"signupDisabled\":false},\"sshGitUrl\":\"git@hf.co\",\"moonHttpUrl\":\"https://huggingface.co\",\"captchaApiKey\":\"bd5f2066-93dc-4bdd-a64b-a24646ca3859\",\"captchaDisabledOnSignup\":true,\"datasetsServerPublicUrl\":\"https://datasets-server.huggingface.co\",\"stripePublicKey\":\"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc\",\"environment\":\"production\",\"userAgent\":\"HuggingFace (production)\"}`);\\n\\t\\t</script>\\n\\n\\t\\t<!-- Stripe -->\\n\\t\\t<script>\\n\\t\\t\\tif ([\"hf.co\", \"huggingface.co\"].includes(window.location.hostname)) {\\n\\t\\t\\t\\tconst script = document.createElement(\"script\");\\n\\t\\t\\t\\tscript.src = \"https://js.stripe.com/v3/\";\\n\\t\\t\\t\\tscript.async = true;\\n\\t\\t\\t\\tdocument.head.appendChild(script);\\n\\t\\t\\t}\\n\\t\\t</script>\\n\\n\\t\\t<!-- Google analytics v4 -->\\n\\t\\t<script>\\n\\t\\t\\tif ([\"hf.co\", \"huggingface.co\"].includes(window.location.hostname)) {\\n\\t\\t\\t\\tconst script = document.createElement(\"script\");\\n\\t\\t\\t\\tscript.src = \"https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL\";\\n\\t\\t\\t\\tscript.async = true;\\n\\t\\t\\t\\tdocument.head.appendChild(script);\\n\\n\\t\\t\\t\\twindow.dataLayer = window.dataLayer || [];\\n\\t\\t\\t\\tfunction gtag() {\\n\\t\\t\\t\\t\\tif (window.dataLayer !== undefined) {\\n\\t\\t\\t\\t\\t\\twindow.dataLayer.push(arguments);\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tgtag(\"js\", new Date());\\n\\t\\t\\t\\tgtag(\"config\", \"G-8Q63TH4CSL\", { page_path: \"/blog/moe\" });\\n\\t\\t\\t\\t/// ^ See https://developers.google.com/analytics/devguides/collection/gtagjs/pages\\n\\t\\t\\t\\tgtag(\"consent\", \"default\", { ad_storage: \"denied\", analytics_storage: \"denied\" });\\n\\t\\t\\t\\t/// ^ See https://developers.google.com/tag-platform/gtagjs/reference#consent\\n\\t\\t\\t\\t/// TODO: ask the user for their consent and update this with gtag(\\'consent\\', \\'update\\')\\n\\t\\t\\t}\\n\\t\\t</script>\\n\\t</body>\\n</html>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document\n",
        "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
      ],
      "metadata": {
        "id": "fMJpa5MdQfO4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the HTML string to a file\n",
        "with open('data/blog.html', \"w\") as file:\n",
        "  file.write(document.text)"
      ],
      "metadata": {
        "id": "HF09nDm-QtRJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Reading from Directory\n",
        "* Using SimpleDirectoryReader you can directly load all files present in the directory or specify the multiple file names you want to read."
      ],
      "metadata": {
        "id": "N4NPKjxfRP2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader"
      ],
      "metadata": {
        "id": "IC-5BeN4RKh3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('./data/').load_data()"
      ],
      "metadata": {
        "id": "LNRmJfTARaRH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load with file names:"
      ],
      "metadata": {
        "id": "XAwe6G0iRiQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(input_files=['/content/data/transformers.pdf',\n",
        "                                               '/content/data/transactions.csv']).load_data()"
      ],
      "metadata": {
        "id": "uejJL8WhRgp5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read documents\n",
        "documents[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "nQ9D6ni9Rvlm",
        "outputId": "f1b7599f-96c2-40d7-d62c-ad26ea0e1435"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5B5nBGtSE8x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}